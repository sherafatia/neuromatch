{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvdru8yBIB6s"
   },
   "source": [
    "**Author:** Arefeh Sherafati\n",
    "\n",
    "**Data loading author:** steeve.laquitaine@epfl.ch: Courtesy of Neuromatch Academy Computational Neuroscience Track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLy9-aMLILcr"
   },
   "source": [
    "## Model Summary: *A Switching Bayesian Observer Model for Human Perceptual Estimation*  \n",
    "**Paper**: [Laquitaine & Gardner, Neuron, 2018](https://doi.org/10.1016/j.neuron.2017.12.011)\n",
    "\n",
    "---\n",
    "\n",
    "### Main Objective\n",
    "To understand how **prior expectations** and **sensory evidence** interact in human perception, especially during motion direction estimation under uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### Experimental Design\n",
    "- **Subjects**: 12 human participants  \n",
    "- **Task**: Motion direction estimation  \n",
    "- **Manipulations**:\n",
    "  - **Sensory uncertainty**: Controlled via motion coherence (low coherence = noisier input)\n",
    "  - **Priors**: Directions drawn from different underlying distributions (some more frequent)\n",
    "\n",
    "- **Data**: Continuous direction estimates across ~83,000 trials\n",
    "\n",
    "---\n",
    "\n",
    "### Key Questions\n",
    "- Do humans combine prior knowledge and sensory input in a **Bayesian-optimal** way?  \n",
    "- Or is behavior better explained by **heuristics**, such as switching between prior and sensory evidence?\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Approach\n",
    "Compared three observer models:\n",
    "- **Bayesian model**: Integrates prior + likelihood into a posterior\n",
    "- **Switching model**: Chooses **either** prior **or** likelihood on each trial\n",
    "- **Weighted mixture model**: Combines both with flexible weighting\n",
    "\n",
    "Fitted models to:\n",
    "- Trial-level behavior (errors and reaction times)  \n",
    "- Used **likelihood-based model comparison** to evaluate fits\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "- Human observers **do not always perform Bayesian integration**.\n",
    "- Behavior is better explained by a **switching observer model**, suggesting:\n",
    "  - People flexibly rely on **prior** or **sensory** information depending on context or confidence\n",
    "- This **switching strategy** yields **more robust behavior** under uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### Broader Implications\n",
    "- Challenges the notion that perception is always **Bayesian**  \n",
    "- Suggests perceptual inference may be **context-dependent**  \n",
    "- Introduces a **novel modeling framework** (switching observer) that better reflects human behavior\n",
    "\n",
    "---\n",
    "\n",
    "### Key Contributions\n",
    "- Introduction of a **switching observer model**  \n",
    "- Collection of a large-scale behavioral dataset (~83k trials)  \n",
    "- Empirical demonstration that switching strategy outperforms strict Bayesian integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ8LYiPcIL5K"
   },
   "outputs": [],
   "source": [
    "# @title Dependencies\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import os, requests\n",
    "from scipy.stats import norm, uniform\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy import pi\n",
    "from copy import copy\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stTcLU2DIPTD"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "rcParams['figure.figsize'] = [20, 4]\n",
    "rcParams['font.size'] = 11\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tj5kxfcwIXIw"
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "url = \"https://github.com/steevelaquitaine/projInference/raw/gh-pages/data/csv/data01_direction4priors.csv\"\n",
    "try:\n",
    "  RequestAPI = requests.get(url)\n",
    "except requests.ConnectionError:\n",
    "  print(\"Failed to download data. Please contact steeve.laquitaine@epfl.ch\")\n",
    "else:\n",
    "  if RequestAPI.status_code != requests.codes.ok:\n",
    "    print(\"Failed to download data. Please contact steeve.laquitaine@epfl.ch\")\n",
    "  else:\n",
    "    with open(\"data01_direction4priors.csv\", \"wb\") as fid:\n",
    "      fid.write(RequestAPI.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "KoHcXNKgJIap",
    "outputId": "b7bde845-0297-4dc8-910b-dc21905da298"
   },
   "outputs": [],
   "source": [
    "# @title Data loading\n",
    "data = pd.read_csv(\"data01_direction4priors.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1dTx1IZInQG"
   },
   "source": [
    "### Data dictionary\n",
    "\n",
    "`data` contains sessions from 12 human subjects, data from [Laquitaine & Gardner, 2018](https://doi.org/10.1016/j.neuron.2017.12.011).\n",
    "\n",
    "Subjects had to estimate the direction of stimulus motion directions.\n",
    "\n",
    "* `data['trial_index']`: trial index\n",
    "* `data['trial_time']`: time at which trial starts with th e central fixation dot\n",
    "* `data['response_arrow_start_angle']`: the angle of the response arrow at the start of the response phase.\n",
    "* `data['motion_direction']`: the stimulus motion direction\n",
    "* `data['motion_coherence']`: the stimulus motion coherence\n",
    "* `data['estimate_x']`: x cartesian coordinate of the stimulus motion direction\n",
    "* `data['estimate_y']`: y cartesian coordinate of the stimulus motion direction\n",
    "* `data['reaction_time']`: subject's reaction time\n",
    "* `data['raw_response_time']`: subject response time since the start of the run (of about 200 trials)\n",
    "* `data['prior_std']`: It is the standard deviation of the statistical distribution (motion direction generative process over trials, which we call \"experimental prior\") from which we sampled the stimulus motion direction displayed in each trial.\n",
    "* `data['prior_mean']`: the most frequently displayed motion direction. It is the mean of the statistical distribution (motion direction generative process over trials, which we call \"experimental prior\") from which we sampled the stimulus motion direction displayed in each trial.\n",
    "* `data['subject_id']`: the id of the subject for which behavior was recorded.\n",
    "* `data['experiment_name']`: the name of the experiment. This dataaset only contains the \"data01_direction4priors\" experiment in which subject underwent a task in which four motion direction were sampled from one of four priors with 10, 40, 60 and 80 degree standard deviations in each block of about 200 trials. The mean of the \"experimental prior\"  was fixed at 225 deg.\n",
    "* `data['experiment_id']`: the id of the experiment.\n",
    "* `data['session_id']`: the id of the session.\n",
    "* `data['run_id']`: the id of the run.\n",
    "\n",
    "\n",
    "The complete original dataset is stored in .mat files here: https://data.mendeley.com/datasets/nxkvtrj9ps/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvfHnHmdIk4j"
   },
   "outputs": [],
   "source": [
    "# @title Utils\n",
    "def build_generative_model(std, mean):\n",
    "    \"\"\"create hidden generative process for motion direction\n",
    "    the experimental Gaussian prior\n",
    "    \"\"\"\n",
    "    # set direction state space\n",
    "    x = np.arange(1, 360, 1)\n",
    "\n",
    "    # calculate probability distribution\n",
    "    pdf = norm.pdf(x, loc=mean, scale=std)\n",
    "    pdf /= sum(pdf)\n",
    "    return pdf\n",
    "\n",
    "def generate_directions(mean, std, n_trials, seed):\n",
    "    return np.round(norm.rvs(loc=mean, scale=std, size=n_trials, random_state=seed))\n",
    "\n",
    "\n",
    "def learn_generative_process(motion_directions, learning_rate, x, n_trials):\n",
    "    \"\"\"learn the generative process\n",
    "    \"\"\"\n",
    "    # set subject initial belief state\n",
    "    initial_prior = uniform.pdf(x, loc=x[0], scale=x[-1])\n",
    "    initial_prior /= sum(initial_prior)\n",
    "    prior = copy.copy(initial_prior)\n",
    "    observed = np.zeros((len(x)))\n",
    "    prediction_errors = []\n",
    "    priors = []\n",
    "\n",
    "    for old_trial in range(0, n_trials):\n",
    "\n",
    "        # locate the observed state component\n",
    "        state_loc = np.where(x == int(motion_directions[old_trial]))[0][0]\n",
    "\n",
    "        # compute its state prediction error\n",
    "        state_pred_error = learning_rate * (1 - prior[state_loc])\n",
    "\n",
    "        # use the prediction error to update the state belief\n",
    "        prior[state_loc] = prior[state_loc] + state_pred_error\n",
    "        prior /= sum(prior)\n",
    "\n",
    "        # tape\n",
    "        prediction_errors.append(state_pred_error)\n",
    "        priors.append(copy.copy(prior))\n",
    "    return priors\n",
    "\n",
    "# circular statistics utils\n",
    "# -------------------\n",
    "def get_cartesian_to_deg(\n",
    "    x: np.ndarray, y: np.ndarray, signed: bool\n",
    ") -> np.ndarray:\n",
    "    \"\"\"convert cartesian coordinates to\n",
    "    angles in degree\n",
    "    Args:\n",
    "        x (np.ndarray): x coordinate\n",
    "        y (np.ndarray): y coordinate\n",
    "        signed (boolean): True (signed) or False (unsigned)\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_cartesian_to_deg\n",
    "            x = np.array([1, 0, -1, 0])\n",
    "            y = np.array([0, 1, 0, -1])\n",
    "            degree = get_cartesian_to_deg(x,y,False)\n",
    "            # Out: array([  0.,  90., 180., 270.])\n",
    "    Returns:\n",
    "        np.ndarray: angles in degree\n",
    "    \"\"\"\n",
    "    # convert to radian (ignoring divide by 0 warning)\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        degree = np.arctan(y / x)\n",
    "\n",
    "    # convert to degree and adjust based\n",
    "    # on quadrant\n",
    "    for ix in range(len(x)):\n",
    "        if (x[ix] >= 0) and (y[ix] >= 0):\n",
    "            degree[ix] = degree[ix] * 180 / np.pi\n",
    "        elif (x[ix] == 0) and (y[ix] == 0):\n",
    "            degree[ix] = 0\n",
    "        elif x[ix] < 0:\n",
    "            degree[ix] = degree[ix] * 180 / np.pi + 180\n",
    "        elif (x[ix] >= 0) and (y[ix] < 0):\n",
    "            degree[ix] = degree[ix] * 180 / np.pi + 360\n",
    "\n",
    "    # if needed, convert signed to unsigned\n",
    "    if not signed:\n",
    "        degree[degree < 0] = degree[degree < 0] + 360\n",
    "    return degree\n",
    "\n",
    "def get_deg_to_rad(deg: np.array, signed: bool):\n",
    "    \"\"\"convert angles in degree to radian\n",
    "    Args:\n",
    "        deg (np.array): angles in degree\n",
    "        signed (bool): True (signed) or False (unsigned)\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_deg_to_rad\n",
    "            radians = get_deg_to_rad(np.array([0, 90, 180, 270], True)\n",
    "            Out: array([ 0., 1.57079633, 3.14159265, -1.57079633])\n",
    "    Returns:\n",
    "        np.ndarray: angles in radian\n",
    "    \"\"\"\n",
    "    # get unsigned radians (1:2*pi)\n",
    "    rad = (deg / 360) * 2 * pi\n",
    "\n",
    "    # get signed radians(-pi:pi)\n",
    "    if signed:\n",
    "        rad[deg > 180] = (deg[deg > 180] - 360) * (\n",
    "            2 * pi / 360\n",
    "        )\n",
    "    return rad\n",
    "\n",
    "def get_polar_to_cartesian(\n",
    "    angle: np.ndarray, radius: float, type: str\n",
    ") -> dict:\n",
    "    \"\"\"convert angle in degree or radian to cartesian coordinates\n",
    "    Args:\n",
    "        angle (np.ndarray): angles in degree or radian\n",
    "        radius (float): radius\n",
    "        type (str): \"polar\" or \"radian\"\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_polar_to_cartesian\n",
    "            degree = np.array([0, 90, 180, 270])\n",
    "            cartesian = get_polar_to_cartesian(degree, 1, \"polar\")\n",
    "            cartesian.keys()\n",
    "\n",
    "            # Out: dict_keys(['deg', 'rad', 'cart'])\n",
    "\n",
    "            cartesian[\"cart\"]\n",
    "\n",
    "            # Out: array([[ 1.,  0.],\n",
    "            #            [ 0.,  1.],\n",
    "            #            [-1.,  0.],\n",
    "            #            [-0., -1.]])\n",
    "    Returns:\n",
    "        dict: _description_\n",
    "    \"\"\"\n",
    "    # convert to radian if needed\n",
    "    theta = dict()\n",
    "    if type == \"polar\":\n",
    "        theta[\"deg\"] = angle\n",
    "        theta[\"rad\"] = angle * np.pi / 180\n",
    "    elif type == \"radian\":\n",
    "        theta[\"deg\"] = get_deg_to_rad(angle, False)\n",
    "        theta[\"rad\"] = angle\n",
    "\n",
    "    # convert to cartesian coordinates\n",
    "    x = radius * np.cos(theta[\"rad\"])\n",
    "    y = radius * np.sin(theta[\"rad\"])\n",
    "\n",
    "    # round to 10e-4\n",
    "    x = np.round(x, 4)\n",
    "    y = np.round(y, 4)\n",
    "\n",
    "    # reshape as (N angles x 2 coord)\n",
    "    theta[\"cart\"] = np.vstack([x, y]).T\n",
    "    return theta\n",
    "\n",
    "def get_circ_weighted_mean_std(\n",
    "    angle: np.ndarray, proba: np.ndarray, type: str\n",
    ") -> dict:\n",
    "    \"\"\"calculate circular data statistics\n",
    "    Args:\n",
    "        angle (np.ndarray): angles in degree or cartesian coordinates\n",
    "        proba (np.ndarray): each angle's probability of occurrence\n",
    "        type (str): \"polar\" or \"cartesian\"\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_circ_weighted_mean_std\n",
    "            degree = np.array([358, 0, 2, 88, 90, 92])\n",
    "            proba = np.array([1, 1, 1, 1, 1, 1])/6\n",
    "            output = get_circ_weighted_mean_std(degree, proba, \"polar\")\n",
    "            output.keys()\n",
    "            # Out: dict_keys(['coord_all', 'deg_all', 'coord_mean', 'deg_mean',\n",
    "            #               'deg_all_for_std', 'deg_mean_for_std', 'deg_var',\n",
    "            #               'deg_std', 'deg_sem'])\n",
    "            output[\"deg_mean\"]\n",
    "            # Out: array([45.])\n",
    "            output[\"deg_std\"]\n",
    "            # array([45.02961988])\n",
    "    Returns:\n",
    "        (dict): angle summary statistics (mean, std, var, sem)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: type is not \"polar\" or \"cartesian\"\n",
    "    \"\"\"\n",
    "\n",
    "    angle = angle.copy()\n",
    "\n",
    "    # if polar, convert to cartesian\n",
    "    if type == \"polar\":\n",
    "        radius = 1\n",
    "        coord = get_polar_to_cartesian(\n",
    "            angle, radius=radius, type=\"polar\"\n",
    "        )\n",
    "    elif type == \"cartesian\":\n",
    "        coord = angle\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"\"\" \"type\" can either be \"polar\" or \"cartesian\" value \"\"\"\n",
    "        )\n",
    "\n",
    "    # store angles\n",
    "    data = dict()\n",
    "    data[\"coord_all\"] = coord[\"cart\"]\n",
    "    data[\"deg_all\"] = coord[\"deg\"]\n",
    "\n",
    "    # calculate mean\n",
    "    # ..............\n",
    "    proba_for_mean = np.tile(proba[:, None], 2)\n",
    "    data[\"coord_mean\"] = np.sum(\n",
    "        proba_for_mean * data[\"coord_all\"], 0\n",
    "    )\n",
    "    data[\"coord_mean\"] = data[\"coord_mean\"][:, None]\n",
    "    data[\"deg_mean\"] = get_cartesian_to_deg(\n",
    "        data[\"coord_mean\"][0],\n",
    "        data[\"coord_mean\"][1],\n",
    "        signed=False,\n",
    "    )\n",
    "\n",
    "    # calculate std\n",
    "    # ..............\n",
    "    n_data = len(data[\"deg_all\"])\n",
    "    data[\"deg_all_for_std\"] = data[\"deg_all\"]\n",
    "    data[\"deg_mean_for_std\"] = np.tile(\n",
    "        data[\"deg_mean\"], n_data\n",
    "    )\n",
    "\n",
    "    # apply corrections\n",
    "    # when 0 <= mean <= 180\n",
    "    if data[\"deg_mean\"] + 180 <= 360:\n",
    "        for ix in range(n_data):\n",
    "            if (\n",
    "                data[\"deg_all\"][ix]\n",
    "                >= data[\"deg_mean\"] + 180\n",
    "            ):\n",
    "                data[\"deg_all_for_std\"][ix] = (\n",
    "                    data[\"deg_all\"][ix] - 360\n",
    "                )\n",
    "    else:\n",
    "        # when 180 <= mean <= 360\n",
    "        for ix in range(n_data):\n",
    "            if (\n",
    "                data[\"deg_all\"][ix]\n",
    "                <= data[\"deg_mean\"] - 180\n",
    "            ):\n",
    "                data[\"deg_mean_for_std\"][ix] = (\n",
    "                    data[\"deg_mean\"] - 360\n",
    "                )\n",
    "\n",
    "    # calculate variance, standard deviation and\n",
    "    # standard error to the mean\n",
    "    data[\"deg_var\"] = np.array(\n",
    "        [\n",
    "            sum(\n",
    "                proba\n",
    "                * (\n",
    "                    data[\"deg_all_for_std\"]\n",
    "                    - data[\"deg_mean_for_std\"]\n",
    "                )\n",
    "                ** 2\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    data[\"deg_std\"] = np.sqrt(data[\"deg_var\"])\n",
    "    data[\"deg_sem\"] = data[\"deg_std\"] / np.sqrt(n_data)\n",
    "    return data\n",
    "\n",
    "def get_signed_angle(\n",
    "    origin: np.ndarray, destination: np.ndarray, type: str\n",
    "):\n",
    "    \"\"\"get the signed angle difference between origin and destination angles\n",
    "    Args:\n",
    "        origin (np.ndarray): origin angle\n",
    "        destination (np.ndarray): destination angle\n",
    "        type (str): angle type (\"polar\", \"radian\", \"cartesian\")\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            angle = get_signed_angle(90, 45, 'polar')\n",
    "\n",
    "            # Out: array([45.])\n",
    "\n",
    "            angle = get_signed_angle(90, 45, 'radian')\n",
    "            # Out: array([58.3103779])\n",
    "            origin = np.array([[0, 1]])\n",
    "            destination = np.array([[1, 0]])\n",
    "            angle = get_signed_angle(origin, destination, \"cartesian\")\n",
    "\n",
    "            # Out: array([90.])\n",
    "    Returns:\n",
    "        (np.ndarray): signed angle differences\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to cartesian coordinates\n",
    "    if type == \"polar\" or type == \"radian\":\n",
    "        origin_dict = get_polar_to_cartesian(\n",
    "            origin, radius=1, type=type\n",
    "        )\n",
    "        destination_dict = get_polar_to_cartesian(\n",
    "            destination, radius=1, type=type\n",
    "        )\n",
    "    elif type == \"cartesian\":\n",
    "        origin_dict = dict()\n",
    "        destination_dict = dict()\n",
    "        origin_dict[\"cart\"] = origin\n",
    "        destination_dict[\"cart\"] = destination\n",
    "\n",
    "    # get coordinates\n",
    "    xV1 = origin_dict[\"cart\"][:, 0]\n",
    "    yV1 = origin_dict[\"cart\"][:, 1]\n",
    "    xV2 = destination_dict[\"cart\"][:, 0]\n",
    "    yV2 = destination_dict[\"cart\"][:, 1]\n",
    "\n",
    "    # Calculate the angle separating the\n",
    "    # two vectors in degrees\n",
    "    angle = -(180 / np.pi) * np.arctan2(\n",
    "        xV1 * yV2 - yV1 * xV2, xV1 * xV2 + yV1 * yV2\n",
    "    )\n",
    "    return angle\n",
    "\n",
    "def get_combination_set(database: np.ndarray):\n",
    "    \"\"\"get the set of row combinations\n",
    "\n",
    "    Args:\n",
    "        database (np.ndarray): an N-D array\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray, np.ndarray): `combs` is the set\n",
    "        of combinations, `b` are the row indices for each combination\n",
    "        in database, `c` are the rows indices for each combination in\n",
    "        combs.\n",
    "    \"\"\"\n",
    "    combs, ia, ic = np.unique(\n",
    "        database,\n",
    "        return_index=True,\n",
    "        return_inverse=True,\n",
    "        axis=0,\n",
    "    )\n",
    "    return (combs, ia, ic)\n",
    "\n",
    "def get_data_stats(data: pd.Series, output: dict):\n",
    "    \"\"\"calculate data statistics\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): stimulus feature estimates\n",
    "        output (dict): ::\n",
    "\n",
    "            'PestimateGivenModel': estimate probabilities\n",
    "            'map': max-a-posteriori percepts\n",
    "            'conditions': task conditions\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns data mean and std to output\n",
    "    \"\"\"\n",
    "    # get conditions\n",
    "    cond = output[\"conditions\"]\n",
    "\n",
    "    # initialise statistics\n",
    "    data_mean = []\n",
    "    data_std = []\n",
    "\n",
    "    # get set of conditions\n",
    "    cond_set, ix, _ = get_combination_set(cond)\n",
    "\n",
    "    # record stats by condition\n",
    "    for c_i in range(len(cond_set)):\n",
    "\n",
    "        # find condition's instances\n",
    "        loc_1 = cond[:, 0] == cond_set[c_i, 0]\n",
    "        loc_2 = cond[:, 1] == cond_set[c_i, 1]\n",
    "        loc_3 = cond[:, 2] == cond_set[c_i, 2]\n",
    "\n",
    "        # get associated data\n",
    "        data_c_i = data.values[loc_1 & loc_2 & loc_3]\n",
    "\n",
    "        # set each instance with equal probability\n",
    "        trial_proba = np.tile(\n",
    "            1 / len(data_c_i), len(data_c_i)\n",
    "        )\n",
    "\n",
    "        # get statistics\n",
    "        stats = get_circ_weighted_mean_std(\n",
    "            data_c_i, trial_proba, type=\"polar\",\n",
    "        )\n",
    "\n",
    "        # record statistics\n",
    "        data_mean.append(stats[\"deg_mean\"])\n",
    "        data_std.append(stats[\"deg_std\"])\n",
    "\n",
    "    # record statistics\n",
    "    output[\"data_mean\"] = np.array(data_mean)\n",
    "    output[\"data_std\"] = np.array(data_std)\n",
    "\n",
    "    # record their condition\n",
    "    output[\"conditions\"] = cond_set\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFVE05f0Isfr"
   },
   "source": [
    "## Hypothesis I: A Basic Linear Regression Model of Circular Distance Prediction\n",
    "\n",
    "**Goal**  \n",
    "Calculate the circular distances between past trial estimates and current trial displayed directions.  \n",
    "- Split the data into train and test sets.  \n",
    "- Train a linear regression to predict the error from that feature.  \n",
    "- Report R² on the test set.  \n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- **Circular distances** are computed modulo 360 (wrapped angular differences).\n",
    "- The **prediction target** is the **angular error**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{angular error} = \\text{estimate} - \\text{stimulus direction}\n",
    "$$\n",
    "\n",
    "  (wrapped within [–180°, +180°] or [0°, 360°], depending on convention)\n",
    "\n",
    "- The main **feature** is the **circular distance between the previous trial's estimate and the current trial’s stimulus direction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX3vqyMSIv30",
    "outputId": "350a8260-946e-4e5a-819f-ae04b6397efd"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Step 1: Prepare Data ---\n",
    "df = data.copy()\n",
    "\n",
    "# Use existing function from notebook to convert (x, y) to degrees\n",
    "df[\"estimate_deg\"] = get_cartesian_to_deg(df[\"estimate_x\"].values, df[\"estimate_y\"].values, signed=False)\n",
    "\n",
    "# Sort to maintain temporal trial order within each subject\n",
    "df = df.sort_values([\"subject_id\", \"session_id\", \"run_id\", \"trial_index\"])\n",
    "\n",
    "# Create feature: circular distance between previous estimate and current stimulus\n",
    "df[\"prev_estimate\"] = df.groupby(\"subject_id\")[\"estimate_deg\"].shift(1)\n",
    "\n",
    "# Use built-in helper to compute circular distances\n",
    "def circular_distance_deg(a, b):\n",
    "    return np.angle(np.exp(1j * np.deg2rad(a - b)), deg=True)\n",
    "\n",
    "df[\"circ_dist_prev_est_to_stim\"] = circular_distance_deg(df[\"motion_direction\"], df[\"prev_estimate\"])\n",
    "\n",
    "# Create target: circular error = estimate - stimulus direction (wrapped)\n",
    "df[\"circular_error\"] = circular_distance_deg(df[\"estimate_deg\"], df[\"motion_direction\"])\n",
    "\n",
    "# Drop NaNs\n",
    "df = df.dropna(subset=[\"circ_dist_prev_est_to_stim\", \"circular_error\"])\n",
    "\n",
    "# --- Step 2: Split into train/test ---\n",
    "X = df[[\"circ_dist_prev_est_to_stim\"]].values\n",
    "y = df[\"circular_error\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Step 3: Train linear regression ---\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# --- Step 4: Report R² score ---\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² score on test set: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTTT_VweIyBk"
   },
   "source": [
    "### Scatter Plot with Line of Best Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "BbCKVUATI5z-",
    "outputId": "2da9628a-2262-4714-e97a-fd5d74b6ba47"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_test, y_test, alpha=0.3, label=\"True\")\n",
    "plt.scatter(X_test, y_pred, color='red', alpha=0.3, label=\"Predicted\")\n",
    "plt.xlabel(\"Circular Distance: Prev Estimate to Stimulus (deg)\")\n",
    "plt.ylabel(\"Angular Error (deg)\")\n",
    "plt.title(\"Prediction of Angular Error from Circular Distance\")\n",
    "plt.legend()\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo_zKExTJSpB"
   },
   "source": [
    "### Binned Prediction Plot (Mean ± Std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "Fm2EG11GJRUV",
    "outputId": "6fe67d3d-148d-42c5-a238-ba515c3855c4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Bin the X values (e.g. into 20 bins)\n",
    "df_plot = pd.DataFrame({\n",
    "    \"feature\": X_test.flatten(),\n",
    "    \"true\": y_test,\n",
    "    \"pred\": y_pred\n",
    "})\n",
    "\n",
    "bins = np.linspace(-180, 180, 20)\n",
    "df_plot[\"bin\"] = pd.cut(df_plot[\"feature\"], bins)\n",
    "\n",
    "# Compute mean and std per bin\n",
    "mean_true = df_plot.groupby(\"bin\")[\"true\"].mean()\n",
    "mean_pred = df_plot.groupby(\"bin\")[\"pred\"].mean()\n",
    "std_true = df_plot.groupby(\"bin\")[\"true\"].std()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.errorbar(mean_true.index.categories.mid, mean_true, yerr=std_true, fmt='o', label=\"Mean True ± STD\")\n",
    "plt.plot(mean_pred.index.categories.mid, mean_pred, 'r--', label=\"Mean Predicted\")\n",
    "plt.xlabel(\"Binned Circular Distance (deg)\")\n",
    "plt.ylabel(\"Angular Error (deg)\")\n",
    "plt.title(\"Binned Fit of Angular Error vs Circular Distance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA8qZu5YJctS"
   },
   "source": [
    "## Hypothesis I Auxiliary Predictive Test: Effect of Absolute Circular Distance on Estimation Error\n",
    "\n",
    "In our original model, we used the **signed circular distance** between the **previous trial’s estimate** and the **current trial’s stimulus direction** as the predictor. This assumes that the **direction** of the difference (clockwise vs counterclockwise) influences behavior.\n",
    "\n",
    "However, we now hypothesize that **only the magnitude** of the difference matters — not the direction. That is, larger angular separations between past and current stimuli might lead to **grea**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "eSP0nvReJY9i",
    "outputId": "681f29b4-dd11-4957-8cf5-681b5b27e3e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Use absolute value of circular distance as the feature ---\n",
    "df[\"abs_circ_dist\"] = np.abs(df[\"circ_dist_prev_est_to_stim\"])\n",
    "\n",
    "# Drop NaNs\n",
    "df_model = df.dropna(subset=[\"abs_circ_dist\", \"circular_error\"])\n",
    "\n",
    "# Prepare data\n",
    "X = df_model[[\"abs_circ_dist\"]].values\n",
    "y = df_model[\"circular_error\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Report R²\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² score using absolute circular distance: {r2:.3f}\")\n",
    "\n",
    "# Plot using your specified format\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_test, y_test, alpha=0.3, label=\"True\")\n",
    "plt.scatter(X_test, y_pred, color='red', alpha=0.3, label=\"Predicted\")\n",
    "plt.xlabel(\"Circular Distance: Prev Estimate to Stimulus (deg)\")  # Keep original axis label\n",
    "plt.ylabel(\"Angular Error (deg)\")\n",
    "plt.title(\"Prediction of Angular Error from Circular Distance\")\n",
    "plt.legend()\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mECJx7l7JV9O"
   },
   "source": [
    "## Hypothesis I Conclusion: Predictive Power of Circular Distance Features\n",
    "\n",
    "We tested whether the **circular distance between the previous trial’s estimate and the current trial’s stimulus direction** could predict the **angular error** in human motion direction estimation.\n",
    "\n",
    "Two models were compared:\n",
    "- One using the **signed circular distance** as a feature.\n",
    "- One using the **absolute circular distance** as a feature.\n",
    "---\n",
    "### Results\n",
    "- The **signed circular distance** model yielded an R² ≈ 0.01\n",
    "- The **absolute circular distance** model yielded an R² ≈ 0.000\n",
    "---\n",
    "### Interpretation\n",
    "Both models explained **very little to no variance** in the estimation error, suggesting that:\n",
    "- The circular distance between past and current trials alone is **not a strong linear predictor** of angular error.\n",
    "- The relationship may be **nonlinear**, **subject-specific**, or influenced by **additional factors** such as:\n",
    "  - Sensory uncertainty (e.g., motion coherence)\n",
    "  - Prior distribution strength (`prior_std`)\n",
    "  - Decision confidence or reaction time\n",
    "  - Trial history beyond just the immediately previous trial\n",
    "---\n",
    "### Next Steps\n",
    "To better understand human estimation behavior, future models should:\n",
    "- Include **multiple features** in a multivariate regression\n",
    "- Explore **nonlinear models**\n",
    "- Consider subject-specific or hierarchical modeling\n",
    "- Potentially use **Bayesian observer frameworks** that align with prior work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIA4mVJWJhQa"
   },
   "source": [
    "## Hypothesis II: Multivariate Predictors of Estimation Error\n",
    "\n",
    "Based on previous results, we observed that the circular distance between the previous trial’s estimate and the current stimulus direction — whether signed or absolute — is **not a strong standalone predictor** of human angular error in this task.\n",
    "\n",
    "We now hypothesize that:\n",
    "> **A combination of multiple task-relevant features** can better predict the estimation error than any single feature alone.\n",
    "\n",
    "### Features Included:\n",
    "- `abs_circ_dist`: Absolute circular distance between previous estimate and current stimulus\n",
    "- `motion_coherence`: Strength of sensory evidence (higher = less uncertainty)\n",
    "- `prior_std`: Variability in the experimental prior (lower = stronger prior)\n",
    "- `reaction_time`: May reflect decision confidence or task difficulty\n",
    "\n",
    "We will fit a **multiple linear regression model** using these features and evaluate its performance using R² on a held-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "88ZQ2WSuJUZj",
    "outputId": "0f7fb5db-473c-46da-94d8-dec680019e17"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Define features ---\n",
    "features = [\"abs_circ_dist\", \"motion_coherence\", \"prior_std\", \"reaction_time\"]\n",
    "df_model_multi = df.dropna(subset=features + [\"circular_error\"])\n",
    "\n",
    "print(f\"Sample size: {len(df_model_multi)} observations\")\n",
    "print(f\"Missing data removed: {len(df) - len(df_model_multi)} observations\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_model_multi[features].values\n",
    "y = df_model_multi[\"circular_error\"].values\n",
    "\n",
    "# Standardize features for better coefficient interpretation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "baseline_rmse = np.std(y_test)  # RMSE of always predicting mean\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"R² score: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"Baseline RMSE (predicting mean): {baseline_rmse:.3f}\")\n",
    "print(f\"RMSE improvement: {((baseline_rmse - rmse) / baseline_rmse * 100):.1f}%\")\n",
    "\n",
    "# Statistical significance (rough approximation)\n",
    "n = len(y_test)\n",
    "if r2 > 0:\n",
    "    f_stat = (r2 / (1 - r2)) * ((n - len(features) - 1) / len(features))\n",
    "    print(f\"Approximate F-statistic: {f_stat:.3f}\")\n",
    "\n",
    "# --- Enhanced Plot 1: Predicted vs. True values ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.4, s=20)\n",
    "plt.xlabel(\"True Angular Error (deg)\")\n",
    "plt.ylabel(\"Predicted Angular Error (deg)\")\n",
    "plt.title(f\"Predicted vs. True Angular Error (R² = {r2:.3f})\")\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(y_test, y_pred, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test, p(y_test), \"b--\", alpha=0.8, label=f'Actual Trend (slope={z[0]:.3f})')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Residuals vs. Predicted ---\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.4, s=20)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=2, label='Perfect Predictions')\n",
    "plt.xlabel(\"Predicted Angular Error (deg)\")\n",
    "plt.ylabel(\"Residuals (True - Predicted)\")\n",
    "plt.title(\"Residuals vs. Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add LOESS smooth line to detect patterns\n",
    "from scipy import stats\n",
    "if len(y_pred) > 50:  # Only if enough points\n",
    "    # Sort by predicted values for smooth line\n",
    "    sort_idx = np.argsort(y_pred)\n",
    "    y_pred_sorted = y_pred[sort_idx]\n",
    "    residuals_sorted = residuals[sort_idx]\n",
    "\n",
    "    # Simple moving average as LOESS approximation\n",
    "    window = max(20, len(y_pred) // 20)\n",
    "    smoothed = pd.Series(residuals_sorted).rolling(window=window, center=True).mean()\n",
    "    plt.plot(y_pred_sorted, smoothed, 'orange', linewidth=2, label='Trend')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: Standardized Feature Coefficients ---\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": lr.coef_,\n",
    "    \"Abs_Coefficient\": np.abs(lr.coef_)\n",
    "}).sort_values(\"Abs_Coefficient\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red' if x < 0 else 'blue' for x in coef_df[\"Coefficient\"]]\n",
    "plt.barh(coef_df[\"Feature\"], coef_df[\"Coefficient\"], color=colors, alpha=0.7)\n",
    "plt.xlabel(\"Standardized Coefficient\")\n",
    "plt.title(\"Feature Importance (Standardized Coefficients)\")\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Analysis: Feature Correlations ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "feature_corr = df_model_multi[features + [\"circular_error\"]].corr()\n",
    "sns.heatmap(feature_corr, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Statistics ---\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(df_model_multi[features].describe())\n",
    "\n",
    "print(f\"\\nFeature-Target Correlations:\")\n",
    "for feature in features:\n",
    "    corr = df_model_multi[feature].corr(df_model_multi[\"circular_error\"])\n",
    "    print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# --- Diagnostic: Check for outliers ---\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.boxplot(df_model_multi[feature])\n",
    "    plt.title(f\"{feature}\")\n",
    "    plt.xticks([])\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Feature Distributions (Check for Outliers)\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# --- Interpretation Summary ---\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"INTERPRETATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"• Model explains {r2*100:.1f}% of variance in angular error\")\n",
    "print(f\"• RMSE is {rmse:.1f}°, vs baseline of {baseline_rmse:.1f}°\")\n",
    "\n",
    "if r2 < 0.05:\n",
    "    print(\"• POOR MODEL FIT: Linear combination of these features\")\n",
    "    print(\"  does not effectively predict estimation error\")\n",
    "    print(\"• Possible reasons:\")\n",
    "    print(\"  - Non-linear relationships\")\n",
    "    print(\"  - Missing important predictors\")\n",
    "    print(\"  - High individual variability\")\n",
    "    print(\"  - Measurement noise dominates signal\")\n",
    "\n",
    "strongest_predictor = coef_df.loc[coef_df[\"Abs_Coefficient\"].idxmax(), \"Feature\"]\n",
    "print(f\"• Strongest predictor: {strongest_predictor}\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"• Consider non-linear models (polynomial, tree-based)\")\n",
    "print(f\"• Include additional features (individual differences, trial history)\")\n",
    "print(f\"• Check for interaction effects between features\")\n",
    "print(f\"• Consider mixed-effects models to account for individual differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_0qxrwjJl8x"
   },
   "source": [
    "#  Hypothesis II Conclusion:\n",
    "\n",
    "Model Performance\n",
    "\n",
    "The extremely poor performance (R² = 0.001, RMSE = 50.182) indicates that this linear combination of features explains virtually none of the variance in angular estimation error. This suggests several important findings:\n",
    "\n",
    "Linear relationships are insufficient: The features may have non-linear relationships with estimation error, or the relationships may be more complex than a simple additive model can capture.\n",
    "Missing key predictors: The most important factors driving estimation error may not be included in your current feature set.\n",
    "High individual variability: Human estimation error in this task may be dominated by factors not captured in these variables (e.g., attention, fatigue, individual differences in strategy).\n",
    "\n",
    "Feature Effects (from the coefficient plot)\n",
    "\n",
    "Motion coherence has the largest positive coefficient (around 3.5) suggesting higher coherence is associated with larger errors (counterintuitive)\n",
    "Absolute circular distance has a moderate positive effect (around 0.8)\n",
    "Reaction time has a smaller positive effect (around 0.7)\n",
    "Prior standard deviation has minimal effect (near zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh1IjqnIJof6"
   },
   "source": [
    "# Hypothesis III: Bayesian Linear Regression for Estimation Error Prediction\n",
    "\n",
    "Our earlier linear models provided point estimates for feature weights but could not express uncertainty in those estimates. We now hypothesize that:\n",
    "\n",
    "> A **Bayesian linear regression model** can quantify the **uncertainty in feature relationships**, especially for circular distance and other task-related variables.\n",
    "---\n",
    "### Why Bayesian Regression?\n",
    "- It assumes a **prior distribution over the weights** (e.g., Gaussian prior on coefficients).\n",
    "- It returns **posterior distributions**, not just point estimates.\n",
    "- This allows us to measure **confidence** in whether each predictor contributes meaningfully to the prediction.\n",
    "---\n",
    "### Assumptions:\n",
    "We assume:\n",
    "- Angular error is normally distributed around a linear combination of features.\n",
    "- Feature weights come from a zero-mean Gaussian prior.\n",
    "\n",
    "We will use **Bayesian Ridge Regression** from `scikit-learn`, which approximates the Bayesian posterior and provides credible intervals for coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZR4xwJNTJlXv",
    "outputId": "73dca790-74bf-4dca-b136-b0d7c9d5ab78"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Features & target ---\n",
    "features = [\"abs_circ_dist\", \"motion_coherence\", \"prior_std\", \"reaction_time\"]\n",
    "df_model_bayes = df.dropna(subset=features + [\"circular_error\"])\n",
    "\n",
    "X = df_model_bayes[features].values\n",
    "y = df_model_bayes[\"circular_error\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Fit Bayesian Ridge Regression ---\n",
    "bayes_model = BayesianRidge(compute_score=True)\n",
    "bayes_model.fit(X_train, y_train)\n",
    "y_pred = bayes_model.predict(X_test)\n",
    "\n",
    "# --- Evaluation ---\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"R² (Bayesian Ridge): {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "# --- Extract coefficient statistics ---\n",
    "# Get posterior mean of coefficients\n",
    "coef_mean = bayes_model.coef_\n",
    "\n",
    "# Calculate posterior covariance and standard deviations\n",
    "# For Bayesian Ridge, we can estimate the uncertainty using the posterior covariance\n",
    "# Sigma = (alpha * I + beta * X.T @ X)^(-1) where alpha and beta are learned hyperparameters\n",
    "X_train_centered = X_train - np.mean(X_train, axis=0)\n",
    "precision_matrix = bayes_model.alpha_ * np.eye(X_train.shape[1]) + bayes_model.lambda_ * X_train_centered.T @ X_train_centered\n",
    "covariance_matrix = np.linalg.inv(precision_matrix)\n",
    "coef_std = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "# Create DataFrame for coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Mean\": coef_mean,\n",
    "    \"StdDev\": coef_std\n",
    "})\n",
    "\n",
    "print(\"\\nCoefficient Statistics:\")\n",
    "print(coef_df)\n",
    "\n",
    "# --- Plot 1: Predictions vs. True values ---\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.xlabel(\"True Angular Error (deg)\")\n",
    "plt.ylabel(\"Predicted Angular Error (deg)\")\n",
    "plt.title(\"Bayesian Ridge: Predicted vs. True Angular Error\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "# Add diagonal line for perfect predictions\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Residuals vs. Predicted ---\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Angular Error (deg)\")\n",
    "plt.ylabel(\"Residuals (True - Predicted)\")\n",
    "plt.title(\"Residuals vs. Predicted (Bayesian Ridge)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: Coefficients with uncertainty using matplotlib ---\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Horizontal bar plot with error bars\n",
    "plt.barh(\n",
    "    y=coef_df[\"Feature\"],\n",
    "    width=coef_df[\"Mean\"],\n",
    "    xerr=coef_df[\"StdDev\"],\n",
    "    align=\"center\",\n",
    "    alpha=0.7,\n",
    "    color=\"skyblue\",\n",
    "    ecolor=\"black\",\n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Posterior Mean Coefficient\")\n",
    "plt.title(\"Posterior Mean and Uncertainty (Bayesian Ridge)\")\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Additional: Plot coefficient credible intervals ---\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Calculate 95% credible intervals (assuming normal posterior)\n",
    "ci_lower = coef_df[\"Mean\"] - 1.96 * coef_df[\"StdDev\"]\n",
    "ci_upper = coef_df[\"Mean\"] + 1.96 * coef_df[\"StdDev\"]\n",
    "\n",
    "# Create error bar plot\n",
    "plt.errorbar(\n",
    "    x=coef_df[\"Mean\"],\n",
    "    y=range(len(features)),\n",
    "    xerr=1.96 * coef_df[\"StdDev\"],\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    "    capthick=2,\n",
    "    markersize=8\n",
    ")\n",
    "\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Bayesian Ridge Coefficients with 95% Credible Intervals\")\n",
    "plt.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print model hyperparameters ---\n",
    "print(f\"\\nLearned Hyperparameters:\")\n",
    "print(f\"Alpha (precision of noise): {bayes_model.alpha_:.6f}\")\n",
    "print(f\"Lambda (precision of coefficients): {bayes_model.lambda_:.6f}\")\n",
    "print(f\"Log marginal likelihood: {bayes_model.scores_[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "creJXgz0J3fW"
   },
   "source": [
    "# Hypothesis III Conclusion: Bayesian Linear Regression Interpretation\n",
    "\n",
    "- **R² score**: 0.001  \n",
    "- **RMSE**: ~50.19 degrees\n",
    "\n",
    "These results indicate that the model explained **virtually no variance** in the angular error. The RMSE confirms that the predictions are highly inaccurate on average.\n",
    "\n",
    "---\n",
    "#### 1. Predicted vs. True Angular Error:\n",
    "- The predicted values are tightly clustered around 0.\n",
    "- True values span the full range of angular error.\n",
    "- The model is clearly **underfitting** and fails to capture the signal in the data.\n",
    "---\n",
    "#### 2. Posterior Coefficient Estimates:\n",
    "- Most coefficients are close to zero with **large uncertainty bounds**.\n",
    "- `prior_std` shows a slightly positive coefficient, suggesting a **minor effect** on error, consistent with prior knowledge.\n",
    "- Overall, the model indicates **low confidence** in any single feature being predictive.\n",
    "---\n",
    "### Conclusion:\n",
    "\n",
    "- Angular error is **not linearly predictable** using these features.\n",
    "- The Bayesian framework gives useful uncertainty estimates, which reveal weak or unreliable feature effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDtFq-OEJ_MJ"
   },
   "source": [
    "# Hypothesis IV: Bayesian Observer Model — Probabilistic Estimation of Motion Direction\n",
    "\n",
    "To investigate whether subjects’ behavior conforms to the predictions of a **Basic Bayesian observer model** of sensory inference (Laquitaine & Gardner, 2017; Girshick et al., 2011), we implemented a generative model that combines both **sensory evidence** and **prior expectations** to account for perceptual estimates of motion direction.\n",
    "\n",
    "---\n",
    "### Hypothesis\n",
    "\n",
    "Subjects estimate motion direction by integrating:\n",
    "- A **sensory likelihood**, whose precision depends on motion coherence (i.e., dot coherence modulates noise in the sensory representation), and\n",
    "- A **prior distribution** over motion direction, shaped by the task structure or accumulated history (e.g., recent trials or block-wise statistics).\n",
    "\n",
    "These two sources are combined according to Bayes’ rule to form a **posterior distribution**:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{estimate}} \\propto p(\\theta_{\\text{stim}} \\mid x) \\cdot p(\\theta_{\\text{prior}})\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $p(\\theta_{\\text{stim}} \\mid x)$ is the likelihood of the true motion direction given noisy sensory input  \n",
    "- $p(\\theta_{\\text{prior}})$ is the prior distribution over expected motion directions  \n",
    "- $\\hat{\\theta}_{\\text{estimate}}$ is the resulting posterior estimate, which we compare to subject responses\n",
    "---\n",
    "### Model Implementation\n",
    "\n",
    "To simulate Bayesian inference over circular variables, we modeled both the likelihood and prior as **von Mises distributions** (the circular analog of Gaussians). This allowed us to capture uncertainty in both components and vary their respective concentrations (inverse variance):\n",
    "\n",
    "- **Sensory likelihood**: Centered on the true motion direction; concentration $\\kappa$ decreases with lower motion coherence  \n",
    "- **Prior**: Defined over experimental blocks with known statistics; concentration varies per condition or trial (e.g., sharp vs. flat priors)\n",
    "\n",
    "### Key Assumptions\n",
    "\n",
    "- Trial-by-trial estimates arise from the posterior distribution resulting from Bayesian integration  \n",
    "- Lower motion coherence increases uncertainty (wider likelihood)  \n",
    "- Subject behavior reflects the **mean of the posterior distribution**, with variability linked to the combined uncertainty of prior and likelihood\n",
    "\n",
    "### Experimental Design Recap\n",
    "\n",
    "Subjects viewed short (300 ms) random dot motion stimuli while fixating, and reported perceived direction using a paddle wheel interface. No explicit correctness feedback was provided—only a post-response display of the true direction. We varied both:\n",
    "\n",
    "- **Sensory strength**: via motion coherence (e.g., 0–100%)  \n",
    "- **Prior structure**: via block-wise manipulations of direction distributions (e.g., narrow vs. broad priors)\n",
    "\n",
    "### Goal\n",
    "\n",
    "We test whether this **Bayesian observer model** can account for:\n",
    "- The **mean** of subject estimates across conditions  \n",
    "- The **variability** of estimates, modulated by coherence and prior sharpness  \n",
    "- **Trial-by-trial predictions** of human responses under varying uncertainty\n",
    "\n",
    "This implementation provides a normative framework for evaluating human perceptual inference and compares predicted posteriors to actual reported directions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o4rTw1JOJ3A2"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy==1.25.2 pymc==5.10.3 arviz --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8g2uGs0AKCUv",
    "outputId": "842f9813-8e90-4e39-c573-d7c50072c22f"
   },
   "outputs": [],
   "source": [
    "!pip install pymc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307,
     "referenced_widgets": [
      "2ab5efaea38b4ef8905e4e085c6b154c",
      "5f62a094f08c41ba81316fa2ba7ca113"
     ]
    },
    "id": "L-DhVuK_KDqh",
    "outputId": "7be36a42-cec3-4da5-e6ab-6c13ab591442"
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "# --- Prepare subset of trials ---\n",
    "df_obs = df.dropna(subset=[\"motion_direction\", \"prior_mean\", \"prior_std\", \"motion_coherence\", \"estimate_deg\"])\n",
    "df_obs = df_obs.sample(500, random_state=42)  # use subset for speed\n",
    "\n",
    "# --- Convert angles from degrees to radians ---\n",
    "def deg2rad_circular(x):\n",
    "    \"\"\"Convert degrees to radians, handling circular nature\"\"\"\n",
    "    angles_rad = np.deg2rad(x % 360)\n",
    "    # Wrap to [-π, π] range as required by PyMC's VonMises\n",
    "    angles_rad = np.where(angles_rad > np.pi, angles_rad - 2*np.pi, angles_rad)\n",
    "    return angles_rad\n",
    "\n",
    "theta_stim_rad = deg2rad_circular(df_obs[\"motion_direction\"].values)       # true stimulus direction\n",
    "theta_human_est_rad = deg2rad_circular(df_obs[\"estimate_deg\"].values)      # human estimate (observed)\n",
    "\n",
    "# Use fixed prior mean as specified\n",
    "prior_mean = np.deg2rad(225 % 360)\n",
    "# Wrap prior mean to [-π, π] range\n",
    "if prior_mean > np.pi:\n",
    "    prior_mean = prior_mean - 2*np.pi\n",
    "theta_prior_rad = np.full_like(theta_stim_rad, prior_mean)  # same prior mean for all trials\n",
    "\n",
    "# --- Convert prior std to von Mises kappa (more robust conversion) ---\n",
    "prior_std_deg = np.clip(df_obs[\"prior_std\"].values, 5, 180)  # clip to reasonable range\n",
    "# Better kappa conversion: kappa ≈ 1/σ² for small σ, but we'll use a more robust formula\n",
    "prior_std_rad = np.deg2rad(prior_std_deg)\n",
    "kappa_prior = 1.0 / (prior_std_rad ** 2)  # Simple approximation\n",
    "kappa_prior = np.clip(kappa_prior, 0.1, 50)  # Clip to reasonable range\n",
    "\n",
    "# --- Convert coherence to likelihood precision ---\n",
    "coherence = np.clip(df_obs[\"motion_coherence\"].values, 0.01, 1.0)  # ensure positive\n",
    "kappa_likelihood = coherence * 10 + 0.5  # more conservative scaling\n",
    "\n",
    "# Print some diagnostics\n",
    "print(f\"Kappa prior range: {kappa_prior.min():.2f} to {kappa_prior.max():.2f}\")\n",
    "print(f\"Kappa likelihood range: {kappa_likelihood.min():.2f} to {kappa_likelihood.max():.2f}\")\n",
    "print(f\"Stimulus angles range: {np.degrees([theta_stim_rad.min(), theta_stim_rad.max()])}\")\n",
    "print(f\"Human estimate angles range: {np.degrees([theta_human_est_rad.min(), theta_human_est_rad.max()])}\")\n",
    "print(f\"Prior mean: {np.degrees(prior_mean):.1f} degrees\")\n",
    "print(f\"Data ranges in radians:\")\n",
    "print(f\"  Stimulus: [{theta_stim_rad.min():.2f}, {theta_stim_rad.max():.2f}]\")\n",
    "print(f\"  Human estimates: [{theta_human_est_rad.min():.2f}, {theta_human_est_rad.max():.2f}]\")\n",
    "\n",
    "with pm.Model() as model:\n",
    "\n",
    "    # Convert arrays to PyTensor tensors\n",
    "    theta_stim = pt.as_tensor_variable(theta_stim_rad)\n",
    "    theta_prior = pt.as_tensor_variable(theta_prior_rad)\n",
    "    kappa_like = pt.as_tensor_variable(kappa_likelihood)\n",
    "    kappa_pri = pt.as_tensor_variable(kappa_prior)\n",
    "\n",
    "    # Posterior mean direction using circular statistics\n",
    "    # Compute weighted sum in Cartesian coordinates, then convert back to angle\n",
    "    cos_post = (kappa_like * pt.cos(theta_stim) + kappa_pri * pt.cos(theta_prior)) / (kappa_like + kappa_pri)\n",
    "    sin_post = (kappa_like * pt.sin(theta_stim) + kappa_pri * pt.sin(theta_prior)) / (kappa_like + kappa_pri)\n",
    "\n",
    "    theta_posterior_mu = pt.arctan2(sin_post, cos_post)\n",
    "\n",
    "    # Posterior concentration (optional, for reference)\n",
    "    kappa_post = pt.sqrt(cos_post**2 + sin_post**2) * (kappa_like + kappa_pri)\n",
    "\n",
    "    # Unknown report precision (how noisy are subject's responses)\n",
    "    # Use better initialization\n",
    "    kappa_report = pm.Exponential(\"kappa_report\", lam=0.5)\n",
    "\n",
    "    # Observed human response is drawn from posterior with some reporting noise\n",
    "    theta_estimate = pm.VonMises(\n",
    "        \"theta_estimate\",\n",
    "        mu=theta_posterior_mu,\n",
    "        kappa=kappa_report,\n",
    "        observed=theta_human_est_rad\n",
    "    )\n",
    "\n",
    "    # Check the model before sampling\n",
    "    print(\"Checking model...\")\n",
    "    try:\n",
    "        # Test point evaluation\n",
    "        test_point = model.initial_point()\n",
    "        logp = model.compile_logp()(test_point)\n",
    "        print(f\"Initial log probability: {logp}\")\n",
    "\n",
    "        if np.isfinite(logp):\n",
    "            print(\"Model looks good, starting sampling...\")\n",
    "            trace = pm.sample(\n",
    "                1000,\n",
    "                tune=1000,\n",
    "                cores=1,\n",
    "                return_inferencedata=True,\n",
    "                target_accept=0.85,  # slightly lower for stability\n",
    "                init=\"adapt_diag\",\n",
    "                random_seed=42\n",
    "            )\n",
    "        else:\n",
    "            print(\"Model has infinite log probability. Running model.debug()...\")\n",
    "            model.debug()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model check: {e}\")\n",
    "        print(\"Running model.debug()...\")\n",
    "        model.debug()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJYzm9PFKF3t"
   },
   "source": [
    "### Plot the results of the Basic Bayesian Observer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b6e886eba3ee4d2a85d3e962789a430b",
      "9cc77e7789a74e3a9bfa69fdcb4e27f3"
     ]
    },
    "id": "QH74OaMAKHUZ",
    "outputId": "8477b8d3-a5d1-4ebd-f049-366518a7be58"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import arviz as az\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "fig_size = (15, 12)\n",
    "\n",
    "# Create a comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=fig_size)\n",
    "fig.suptitle('Bayesian Model Results: Motion Direction Estimation', fontsize=16, y=0.98)\n",
    "\n",
    "# 1. Posterior distribution of kappa_report\n",
    "ax1 = axes[0, 0]\n",
    "az.plot_posterior(trace, var_names=['kappa_report'], ax=ax1)\n",
    "ax1.set_title('Posterior: Report Precision (κ_report)')\n",
    "ax1.set_xlabel('κ_report')\n",
    "\n",
    "# 2. Convert kappa to std deviation for interpretation\n",
    "kappa_samples = trace.posterior['kappa_report'].values.flatten()\n",
    "std_report_deg = np.degrees(1/np.sqrt(kappa_samples))  # approximate conversion\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(std_report_deg, bins=50, alpha=0.7, density=True, color='skyblue', edgecolor='black')\n",
    "ax2.axvline(std_report_deg.mean(), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Mean: {std_report_deg.mean():.1f}°')\n",
    "ax2.set_xlabel('Reporting Noise (degrees)')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.set_title('Estimated Reporting Noise')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Trace plot for convergence diagnostics (manual implementation)\n",
    "ax3 = axes[0, 2]\n",
    "kappa_trace = trace.posterior['kappa_report'].values.flatten()\n",
    "ax3.plot(kappa_trace, alpha=0.7, linewidth=0.5)\n",
    "ax3.set_xlabel('Iteration')\n",
    "ax3.set_ylabel('κ_report')\n",
    "ax3.set_title('Trace Plot: Convergence Check')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Model predictions vs observations\n",
    "with model:\n",
    "    # Get posterior predictions\n",
    "    posterior_pred = pm.sample_posterior_predictive(trace, random_seed=42)\n",
    "\n",
    "# Extract predicted values\n",
    "theta_pred = posterior_pred.posterior_predictive['theta_estimate'].values\n",
    "theta_pred_mean = np.mean(theta_pred, axis=(0, 1))  # Average over chains and draws\n",
    "\n",
    "ax4 = axes[1, 0]\n",
    "# Convert back to degrees for plotting\n",
    "obs_deg = np.degrees(theta_human_est_rad)\n",
    "pred_deg = np.degrees(theta_pred_mean)\n",
    "\n",
    "# Handle circular nature of angles for plotting\n",
    "obs_deg_wrapped = ((obs_deg + 180) % 360) - 180\n",
    "pred_deg_wrapped = ((pred_deg + 180) % 360) - 180\n",
    "\n",
    "ax4.scatter(obs_deg_wrapped, pred_deg_wrapped, alpha=0.6, s=20)\n",
    "ax4.plot([-180, 180], [-180, 180], 'r--', linewidth=2, label='Perfect prediction')\n",
    "ax4.set_xlabel('Observed Estimate (degrees)')\n",
    "ax4.set_ylabel('Model Prediction (degrees)')\n",
    "ax4.set_title('Model Predictions vs Observations')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.set_xlim(-180, 180)\n",
    "ax4.set_ylim(-180, 180)\n",
    "\n",
    "# 5. Residuals analysis\n",
    "residuals_deg = np.degrees(np.arctan2(np.sin(theta_pred_mean - theta_human_est_rad),\n",
    "                                     np.cos(theta_pred_mean - theta_human_est_rad)))\n",
    "\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(residuals_deg, bins=50, alpha=0.7, density=True, color='lightcoral', edgecolor='black')\n",
    "ax5.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "ax5.axvline(np.mean(residuals_deg), color='red', linestyle='--', linewidth=2,\n",
    "           label=f'Mean: {np.mean(residuals_deg):.1f}°')\n",
    "ax5.set_xlabel('Prediction Error (degrees)')\n",
    "ax5.set_ylabel('Density')\n",
    "ax5.set_title('Model Residuals')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Effect of coherence on accuracy\n",
    "coherence_vals = df_obs[\"motion_coherence\"].values\n",
    "abs_errors = np.abs(residuals_deg)\n",
    "\n",
    "ax6 = axes[1, 2]\n",
    "# Bin by coherence for cleaner visualization\n",
    "coherence_bins = np.linspace(coherence_vals.min(), coherence_vals.max(), 10)\n",
    "bin_centers = []\n",
    "bin_errors = []\n",
    "bin_stds = []\n",
    "\n",
    "for i in range(len(coherence_bins)-1):\n",
    "    mask = (coherence_vals >= coherence_bins[i]) & (coherence_vals < coherence_bins[i+1])\n",
    "    if np.sum(mask) > 0:\n",
    "        bin_centers.append((coherence_bins[i] + coherence_bins[i+1]) / 2)\n",
    "        bin_errors.append(np.mean(abs_errors[mask]))\n",
    "        bin_stds.append(np.std(abs_errors[mask]) / np.sqrt(np.sum(mask)))\n",
    "\n",
    "ax6.errorbar(bin_centers, bin_errors, yerr=bin_stds, fmt='o-', capsize=5, capthick=2)\n",
    "ax6.set_xlabel('Motion Coherence')\n",
    "ax6.set_ylabel('Mean Absolute Error (degrees)')\n",
    "ax6.set_title('Prediction Error vs Motion Coherence')\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Number of observations: {len(theta_human_est_rad)}\")\n",
    "print(f\"Prior mean direction: {np.degrees(prior_mean):.1f}°\")\n",
    "print(f\"Prior std range: {prior_std_deg.min():.1f}° to {prior_std_deg.max():.1f}°\")\n",
    "print(f\"Motion coherence range: {coherence_vals.min():.2f} to {coherence_vals.max():.2f}\")\n",
    "print()\n",
    "print(\"ESTIMATED PARAMETERS:\")\n",
    "print(f\"Report precision (κ): {kappa_samples.mean():.2f} ± {kappa_samples.std():.2f}\")\n",
    "print(f\"Report noise (std): {std_report_deg.mean():.1f}° ± {std_report_deg.std():.1f}°\")\n",
    "print()\n",
    "print(\"MODEL FIT:\")\n",
    "print(f\"Mean absolute error: {np.mean(abs_errors):.1f}°\")\n",
    "print(f\"Root mean square error: {np.sqrt(np.mean(residuals_deg**2)):.1f}°\")\n",
    "print(f\"Correlation (obs vs pred): {np.corrcoef(obs_deg_wrapped, pred_deg_wrapped)[0,1]:.3f}\")\n",
    "\n",
    "# Convergence diagnostics\n",
    "print()\n",
    "print(\"CONVERGENCE DIAGNOSTICS:\")\n",
    "print(az.summary(trace, var_names=['kappa_report']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WG_l-2gKKc4"
   },
   "source": [
    "# Hypothesis IV Conclusion: Basic Bayesian Observer Model Performance & Convergence\n",
    "\n",
    "**Optimal Bayesian Integration:**\n",
    "- The model assumes people optimally combine their prior belief (-135°) with sensory evidence, weighting each source by its reliability\n",
    "- Convergence diagnostics show the MCMC sampler successfully explored the parameter space with stable chains around κ_report ≈ 2.5\n",
    "- This suggests the core Bayesian framework is computationally tractable for this dataset\n",
    "\n",
    "**Report Precision:**\n",
    "- The estimated report precision (κ = 2.50 ± 0.14) corresponds to about 36° of noise when people translate their internal estimates into responses\n",
    "- This reporting noise combines motor variability, response uncertainty, and any systematic biases in the response process\n",
    "- The relatively tight posterior distribution indicates this parameter is well-identified by the data\n",
    "---\n",
    "## Response Patterns and Coherence Effects\n",
    "\n",
    "**Coherence-Dependent Performance:**\n",
    "- Clear improvement with higher motion coherence: ~64° error at low coherence vs ~59° error at high coherence\n",
    "- This 5° reduction validates a key Bayesian prediction: more reliable sensory evidence (higher coherence) should improve performance\n",
    "- The effect size is modest but systematic, consistent with the challenging nature of this perceptual task\n",
    "\n",
    "**Systematic Biases:**\n",
    "- Mean residual bias of 33.3° suggests the model systematically underestimates human responses\n",
    "- This could indicate additional cognitive processes not captured by pure Bayesian integration\n",
    "- The bias pattern suggests humans may not be perfectly calibrated to the experimental statistics\n",
    "---\n",
    "## Model Fit Quality\n",
    "\n",
    "**Moderate Predictive Performance:**\n",
    "- Correlation of r = 0.373 between predictions and observations explains about 14% of response variance\n",
    "- Mean absolute error of 60.7° reflects the substantial challenge of this perceptual task\n",
    "- The prediction scatter plot shows range compression: model predicts narrower response range than humans actually produce\n",
    "\n",
    "**Residual Analysis:**\n",
    "- Residuals are roughly centered but show systematic deviations from zero\n",
    "- Large individual trial variability suggests important sources of variance not captured by the model\n",
    "- The distribution pattern indicates the Bayesian framework captures the central tendency but misses response variability\n",
    "---\n",
    "## Cognitive Interpretation\n",
    "\n",
    "**Evidence for Bayesian-Like Processing:**\n",
    "- The coherence effect strongly supports the idea that humans weight sensory evidence by its reliability\n",
    "- The model successfully captures the basic mechanism of prior-likelihood integration\n",
    "- Parameter estimates are reasonable and interpretable within the Bayesian framework\n",
    "\n",
    "**Model Limitations Reveal Non-Bayesian Aspects:**\n",
    "- **Range compression** suggests humans may use non-linear response mappings or have miscalibrated confidence\n",
    "- **Individual differences** not captured: all subjects treated as identical Bayesian integrators\n",
    "- **Large unexplained variance** indicates additional cognitive processes beyond optimal integration\n",
    "---\n",
    "## Key Questions for Further Investigation\n",
    "\n",
    "1. **Individual Differences**: Do subjects vary in their integration strategies or noise levels? The current model assumes homogeneous Bayesian processors.\n",
    "\n",
    "2. **Response Mapping**: Is the 36° \"reporting noise\" actually systematic bias in how people map internal estimates to motor responses?\n",
    "\n",
    "3. **Optimality Assessment**: How does this performance compare to theoretical optimal performance given the task statistics?\n",
    "\n",
    "4. **Alternative Strategies**: The moderate fit (r = 0.373) suggests room for improvement with models incorporating non-Bayesian decision strategies.\n",
    "---\n",
    "## Assessment\n",
    "\n",
    "The Bayesian observer model provides a **principled baseline** that captures the fundamental mechanism of cue integration in this task. The clear coherence effect validates core Bayesian predictions about reliability-weighted integration. However, the moderate fit quality and systematic biases indicate that **human behavior deviates meaningfully from optimal Bayesian integration**, motivating the need for more sophisticated models that account for cognitive limitations, individual differences, or alternative decision strategies like the switching observer model.\n",
    "\n",
    "This establishes the Bayesian model as a useful normative benchmark while highlighting specific ways human perception may be bounded or suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PVKFefM4KTZY"
   },
   "source": [
    "# Hypothesis V: Switching Observer Model — Discrete Strategy Use in Perceptual Estimation\n",
    "\n",
    "Inspired by Laquitaine & Gardner (2018), this model tests whether human perceptual behavior reflects **heuristic switching** between two internal strategies, rather than continuous Bayesian integration.\n",
    "\n",
    "---\n",
    "### Hypothesis\n",
    "\n",
    "Rather than always integrating prior and likelihood, subjects **switch between**:\n",
    "\n",
    "- A **prior-based strategy**: relying solely on expectations from task structure  \n",
    "- A **likelihood-based strategy**: relying solely on current sensory evidence\n",
    "\n",
    "On each trial, the observer selects one of the two strategies with some probability. The reported direction is then drawn from a noisy von Mises distribution centered on the selected strategy.\n",
    "\n",
    "---\n",
    "### Switching Rule (Logistic Function)\n",
    "\n",
    "The probability of relying on the prior decreases with increasing sensory reliability (i.e., motion coherence). This switching behavior is modeled as:\n",
    "\n",
    "$$\n",
    "p_{\\text{prior}} = \\sigma(\\alpha \\cdot c + \\beta) = \\frac{1}{1 + \\exp(-(\\alpha \\cdot c + \\beta))}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $c$ is the motion coherence (stimulus reliability)  \n",
    "- $\\alpha$ is the **coherence sensitivity** parameter  \n",
    "- $\\beta$ is the **baseline bias** toward the prior  \n",
    "- $\\sigma(\\cdot)$ is the logistic (sigmoid) function  \n",
    "- $p_{\\text{prior}}$ is the probability of using the **prior strategy** (vs. likelihood)\n",
    "---\n",
    "### Response Model\n",
    "\n",
    "Let $\\theta_{\\text{stim}}$ be the true motion direction, and $\\theta_{\\text{prior}}$ be the prior mean direction. Then the subject’s estimate $\\hat{\\theta}$ is drawn from:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{\\text{estimate}} \\sim\n",
    "\\begin{cases}\n",
    "\\text{VonMises}(\\theta_{\\text{prior}}, \\kappa_{\\text{prior}}) & \\text{with probability } p_{\\text{prior}} \\\\\n",
    "\\text{VonMises}(\\theta_{\\text{stim}}, \\kappa_{\\text{likelihood}}) & \\text{with probability } 1 - p_{\\text{prior}}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\kappa_{\\text{prior}}$ and $\\kappa_{\\text{likelihood}}$ are the report precisions (inverse noise) when using each strategy  \n",
    "- These are **learned from data**, and represent motor or decision noise under each condition.\n",
    "---\n",
    "\n",
    "### Model Implementation\n",
    "\n",
    "In PyMC, we build a **mixture model** of two von Mises distributions per trial, with:\n",
    "\n",
    "- Data-driven priors on $\\alpha$, $\\beta$, $\\kappa_{\\text{prior}}$, $\\kappa_{\\text{likelihood}}$  \n",
    "- Trial-specific mixing weights based on motion coherence  \n",
    "- Posterior inference via NUTS sampling\n",
    "\n",
    "---\n",
    "\n",
    "### Key Assumptions\n",
    "\n",
    "- Subjects alternate between using **prior knowledge** and **sensory evidence**, not both simultaneously  \n",
    "- Strategy choice is probabilistic and coherence-dependent  \n",
    "- Report variability reflects both motor noise and uncertainty tied to the chosen strategy  \n",
    "\n",
    "---\n",
    "\n",
    "### Goal\n",
    "\n",
    "We evaluate whether the **Switching Observer Model** can better explain human responses than the Bayesian model by capturing:\n",
    "\n",
    "- **Bimodal distributions** in responses  \n",
    "- **Nonlinear effects** of coherence on estimation behavior  \n",
    "- **Trial-by-trial prediction errors**, especially in low-coherence conditions\n",
    "\n",
    "By comparing with the Bayesian integration model (Hypothesis IV), we test whether subjects' behavior is better described by a **discrete switching heuristic** rather than continuous probabilistic inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "cbd528fe39574931ad57e3d482f065d8",
      "b5ff38518b884ca483c8fc5482e090f0",
      "7ea7a66ba4004dff846001a6e7302cf3",
      "ced9f21141cf48a78a9f1273669ff321"
     ]
    },
    "id": "jom7X61aKI79",
    "outputId": "1298c29b-d26a-4f16-bfab-a03b75b4e949"
   },
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the same data preparation as before\n",
    "print(\"Preparing data for switching observer model...\")\n",
    "\n",
    "# --- Convert angles from degrees to radians (same as before) ---\n",
    "def deg2rad_circular(x):\n",
    "    \"\"\"Convert degrees to radians, handling circular nature\"\"\"\n",
    "    angles_rad = np.deg2rad(x % 360)\n",
    "    # Wrap to [-π, π] range as required by PyMC's VonMises\n",
    "    angles_rad = np.where(angles_rad > np.pi, angles_rad - 2*np.pi, angles_rad)\n",
    "    return angles_rad\n",
    "\n",
    "theta_stim_rad = deg2rad_circular(df_obs[\"motion_direction\"].values)       # true stimulus direction\n",
    "theta_human_est_rad = deg2rad_circular(df_obs[\"estimate_deg\"].values)      # human estimate (observed)\n",
    "\n",
    "# Use fixed prior mean as specified\n",
    "prior_mean = np.deg2rad(225 % 360)\n",
    "# Wrap prior mean to [-π, π] range\n",
    "if prior_mean > np.pi:\n",
    "    prior_mean = prior_mean - 2*np.pi\n",
    "theta_prior_rad = np.full_like(theta_stim_rad, prior_mean)  # same prior mean for all trials\n",
    "\n",
    "# --- Convert prior std to von Mises kappa (same as before) ---\n",
    "prior_std_deg = np.clip(df_obs[\"prior_std\"].values, 5, 180)  # clip to reasonable range\n",
    "prior_std_rad = np.deg2rad(prior_std_deg)\n",
    "kappa_prior = 1.0 / (prior_std_rad ** 2)  # Simple approximation\n",
    "kappa_prior = np.clip(kappa_prior, 0.1, 50)  # Clip to reasonable range\n",
    "\n",
    "# --- Convert coherence to likelihood precision (same as before) ---\n",
    "coherence = np.clip(df_obs[\"motion_coherence\"].values, 0.01, 1.0)  # ensure positive\n",
    "kappa_likelihood = coherence * 10 + 0.5  # more conservative scaling\n",
    "\n",
    "print(f\"Data prepared: {len(theta_human_est_rad)} trials\")\n",
    "print(f\"Prior mean: {np.degrees(prior_mean):.1f}°\")\n",
    "\n",
    "# =============================================================================\n",
    "# SWITCHING OBSERVER MODEL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nBuilding switching observer model...\")\n",
    "\n",
    "with pm.Model() as switching_model:\n",
    "\n",
    "    # Convert arrays to PyTensor tensors\n",
    "    theta_stim = pt.as_tensor_variable(theta_stim_rad)\n",
    "    theta_prior = pt.as_tensor_variable(theta_prior_rad)\n",
    "    kappa_like = pt.as_tensor_variable(kappa_likelihood)\n",
    "    kappa_pri = pt.as_tensor_variable(kappa_prior)\n",
    "\n",
    "    # === SWITCHING OBSERVER PARAMETERS ===\n",
    "\n",
    "    # Probability of using the prior (vs. using likelihood)\n",
    "    # This varies with coherence - higher coherence = more likely to use likelihood\n",
    "    # We'll model this as a logistic function of coherence\n",
    "    alpha_coherence = pm.Normal(\"alpha_coherence\", mu=0, sigma=2)  # coherence effect strength\n",
    "    beta_coherence = pm.Normal(\"beta_coherence\", mu=0, sigma=2)    # baseline switching tendency\n",
    "\n",
    "    # Probability of using prior (switching probability)\n",
    "    coherence_tensor = pt.as_tensor_variable(coherence)\n",
    "    logit_p_prior = alpha_coherence * coherence_tensor + beta_coherence\n",
    "    p_prior = pm.Deterministic(\"p_prior\", pt.sigmoid(logit_p_prior))\n",
    "\n",
    "    # Report precision when using prior vs likelihood\n",
    "    kappa_report_prior = pm.Exponential(\"kappa_report_prior\", lam=0.5)     # noise when using prior\n",
    "    kappa_report_likelihood = pm.Exponential(\"kappa_report_likelihood\", lam=0.5)  # noise when using likelihood\n",
    "\n",
    "    # === MIXTURE MODEL ===\n",
    "\n",
    "    # For each trial, the response comes from either:\n",
    "    # 1. Prior + noise (with probability p_prior)\n",
    "    # 2. Likelihood + noise (with probability 1 - p_prior)\n",
    "\n",
    "    # Component 1: Using prior (ignoring sensory evidence)\n",
    "    prior_component = pm.VonMises.dist(\n",
    "        mu=theta_prior,\n",
    "        kappa=kappa_report_prior\n",
    "    )\n",
    "\n",
    "    # Component 2: Using likelihood/stimulus (ignoring prior)\n",
    "    likelihood_component = pm.VonMises.dist(\n",
    "        mu=theta_stim,\n",
    "        kappa=kappa_report_likelihood\n",
    "    )\n",
    "\n",
    "    # Mixture of the two strategies\n",
    "    theta_estimate = pm.Mixture(\n",
    "        \"theta_estimate\",\n",
    "        w=pt.stack([p_prior, 1 - p_prior], axis=1),  # mixing weights\n",
    "        comp_dists=[prior_component, likelihood_component],\n",
    "        observed=theta_human_est_rad\n",
    "    )\n",
    "\n",
    "    # Check the model before sampling\n",
    "    print(\"Checking switching model...\")\n",
    "    try:\n",
    "        test_point = switching_model.initial_point()\n",
    "        logp = switching_model.compile_logp()(test_point)\n",
    "        print(f\"Initial log probability: {logp}\")\n",
    "\n",
    "        if np.isfinite(logp):\n",
    "            print(\"Switching model looks good, starting sampling...\")\n",
    "            # Sample with compute_log_likelihood=True for model comparison\n",
    "            switching_trace = pm.sample(\n",
    "                1000,\n",
    "                tune=1000,\n",
    "                cores=1,\n",
    "                return_inferencedata=True,\n",
    "                target_accept=0.85,\n",
    "                init=\"adapt_diag\",\n",
    "                random_seed=42,\n",
    "                idata_kwargs={\"log_likelihood\": True}  # Enable log-likelihood computation\n",
    "            )\n",
    "        else:\n",
    "            print(\"Switching model has infinite log probability. Running model.debug()...\")\n",
    "            switching_model.debug()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during switching model check: {e}\")\n",
    "        print(\"Running model.debug()...\")\n",
    "        switching_model.debug()\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL COMPARISON AND VISUALIZATION\n",
    "# =============================================================================\n",
    "\n",
    "def compare_models(bayesian_trace, switching_trace, bayesian_model, switching_model):\n",
    "    \"\"\"Compare the Bayesian integration model with the switching observer model\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    try:\n",
    "        # Model comparison using WAIC and LOO\n",
    "        with bayesian_model:  # Bayesian integration model\n",
    "            bayesian_waic = az.waic(bayesian_trace)\n",
    "            print(\"Computing LOO for Bayesian model...\")\n",
    "            bayesian_loo = az.loo(bayesian_trace)\n",
    "\n",
    "        with switching_model:  # Switching observer model\n",
    "            switching_waic = az.waic(switching_trace)\n",
    "            print(\"Computing LOO for Switching model...\")\n",
    "            switching_loo = az.loo(switching_trace)\n",
    "\n",
    "        # Compare models using both WAIC and LOO\n",
    "        print(\"\\nWAIC Comparison (lower is better):\")\n",
    "        waic_comparison = az.compare({'Bayesian Integration': bayesian_waic,\n",
    "                                    'Switching Observer': switching_waic})\n",
    "        print(waic_comparison)\n",
    "\n",
    "        print(\"\\nLOO Comparison (lower ELPD_loo is better):\")\n",
    "        loo_comparison = az.compare({'Bayesian Integration': bayesian_loo,\n",
    "                                   'Switching Observer': switching_loo})\n",
    "        print(loo_comparison)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in model comparison: {e}\")\n",
    "        print(\"Proceeding with parameter analysis...\")\n",
    "\n",
    "    # Extract parameters from switching model\n",
    "    alpha_samples = switching_trace.posterior['alpha_coherence'].values.flatten()\n",
    "    beta_samples = switching_trace.posterior['beta_coherence'].values.flatten()\n",
    "    kappa_prior_samples = switching_trace.posterior['kappa_report_prior'].values.flatten()\n",
    "    kappa_likelihood_samples = switching_trace.posterior['kappa_report_likelihood'].values.flatten()\n",
    "\n",
    "    print(f\"\\nSWITCHING MODEL PARAMETERS:\")\n",
    "    print(f\"Coherence effect (α): {alpha_samples.mean():.3f} ± {alpha_samples.std():.3f}\")\n",
    "    print(f\"Baseline switching (β): {beta_samples.mean():.3f} ± {beta_samples.std():.3f}\")\n",
    "    print(f\"Prior report precision: {kappa_prior_samples.mean():.2f} ± {kappa_prior_samples.std():.2f}\")\n",
    "    print(f\"Likelihood report precision: {kappa_likelihood_samples.mean():.2f} ± {kappa_likelihood_samples.std():.2f}\")\n",
    "\n",
    "    # Calculate implied switching probabilities\n",
    "    coherence_range = np.linspace(coherence.min(), coherence.max(), 100)\n",
    "    p_prior_mean = 1 / (1 + np.exp(-(alpha_samples.mean() * coherence_range + beta_samples.mean())))\n",
    "\n",
    "    print(f\"\\nImplied switching behavior:\")\n",
    "    print(f\"  At low coherence ({coherence.min():.2f}): {p_prior_mean[0]:.1%} use prior\")\n",
    "    print(f\"  At high coherence ({coherence.max():.2f}): {p_prior_mean[-1]:.1%} use prior\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    fig.suptitle('Switching Observer Model Results', fontsize=16)\n",
    "\n",
    "    # 1. Switching probability as function of coherence\n",
    "    ax1 = axes[0, 0]\n",
    "\n",
    "    # Plot individual posterior samples (uncertainty)\n",
    "    for i in range(0, len(alpha_samples), 50):  # subsample for visualization\n",
    "        p_prior_sample = 1 / (1 + np.exp(-(alpha_samples[i] * coherence_range + beta_samples[i])))\n",
    "        ax1.plot(coherence_range, p_prior_sample, 'b-', alpha=0.1)\n",
    "\n",
    "    # Plot mean\n",
    "    ax1.plot(coherence_range, p_prior_mean, 'r-', linewidth=3, label='Mean')\n",
    "    ax1.set_xlabel('Motion Coherence')\n",
    "    ax1.set_ylabel('Probability of Using Prior')\n",
    "    ax1.set_title('Switching Probability vs Coherence')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Parameter posteriors\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.hist(alpha_samples, bins=50, alpha=0.7, density=True, label='α (coherence effect)')\n",
    "    ax2.axvline(alpha_samples.mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax2.set_xlabel('α (coherence effect)')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Coherence Effect Parameter')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    ax3 = axes[0, 2]\n",
    "    ax3.hist(beta_samples, bins=50, alpha=0.7, density=True, label='β (baseline)', color='orange')\n",
    "    ax3.axvline(beta_samples.mean(), color='red', linestyle='--', linewidth=2)\n",
    "    ax3.set_xlabel('β (baseline)')\n",
    "    ax3.set_ylabel('Density')\n",
    "    ax3.set_title('Baseline Switching Parameter')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Report precision comparison\n",
    "    ax4 = axes[1, 0]\n",
    "    ax4.hist(1/np.sqrt(kappa_prior_samples), bins=50, alpha=0.7, density=True,\n",
    "             label='Prior noise', color='green')\n",
    "    ax4.hist(1/np.sqrt(kappa_likelihood_samples), bins=50, alpha=0.7, density=True,\n",
    "             label='Likelihood noise', color='purple')\n",
    "    ax4.set_xlabel('Report Noise (std, radians)')\n",
    "    ax4.set_ylabel('Density')\n",
    "    ax4.set_title('Report Noise by Strategy')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. Data fit comparison\n",
    "    with switching_model:\n",
    "        switching_pred = pm.sample_posterior_predictive(switching_trace, random_seed=42)\n",
    "\n",
    "    # Extract predictions\n",
    "    theta_pred_switching = switching_pred.posterior_predictive['theta_estimate'].values\n",
    "    theta_pred_switching_mean = np.mean(theta_pred_switching, axis=(0, 1))\n",
    "\n",
    "    ax5 = axes[1, 1]\n",
    "    obs_deg = np.degrees(theta_human_est_rad)\n",
    "    pred_deg_switching = np.degrees(theta_pred_switching_mean)\n",
    "\n",
    "    # Handle circular nature\n",
    "    obs_deg_wrapped = ((obs_deg + 180) % 360) - 180\n",
    "    pred_deg_switching_wrapped = ((pred_deg_switching + 180) % 360) - 180\n",
    "\n",
    "    ax5.scatter(obs_deg_wrapped, pred_deg_switching_wrapped, alpha=0.6, s=20)\n",
    "    ax5.plot([-180, 180], [-180, 180], 'r--', linewidth=2, label='Perfect prediction')\n",
    "    ax5.set_xlabel('Observed Estimate (degrees)')\n",
    "    ax5.set_ylabel('Switching Model Prediction (degrees)')\n",
    "    ax5.set_title('Switching Model: Predictions vs Observations')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # Calculate correlation\n",
    "    corr_switching = np.corrcoef(obs_deg_wrapped, pred_deg_switching_wrapped)[0,1]\n",
    "    ax5.text(0.05, 0.95, f'r = {corr_switching:.3f}', transform=ax5.transAxes,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    # 5. Residuals comparison\n",
    "    residuals_switching = np.degrees(np.arctan2(\n",
    "        np.sin(theta_pred_switching_mean - theta_human_est_rad),\n",
    "        np.cos(theta_pred_switching_mean - theta_human_est_rad)\n",
    "    ))\n",
    "\n",
    "    ax6 = axes[1, 2]\n",
    "    ax6.hist(residuals_switching, bins=50, alpha=0.7, density=True,\n",
    "             color='lightcoral', edgecolor='black', label='Switching Model')\n",
    "    ax6.axvline(0, color='black', linestyle='-', linewidth=2)\n",
    "    ax6.axvline(np.mean(residuals_switching), color='red', linestyle='--', linewidth=2,\n",
    "               label=f'Mean: {np.mean(residuals_switching):.1f}°')\n",
    "    ax6.set_xlabel('Prediction Error (degrees)')\n",
    "    ax6.set_ylabel('Density')\n",
    "    ax6.set_title('Switching Model Residuals')\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return switching_pred, corr_switching\n",
    "\n",
    "def analyze_switching_behavior(switching_trace, coherence, theta_human_est_rad, theta_stim_rad, theta_prior_rad):\n",
    "    \"\"\"Analyze the switching behavior in more detail\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DETAILED SWITCHING ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Extract parameters\n",
    "    alpha_samples = switching_trace.posterior['alpha_coherence'].values.flatten()\n",
    "    beta_samples = switching_trace.posterior['beta_coherence'].values.flatten()\n",
    "    p_prior_samples = switching_trace.posterior['p_prior'].values  # Shape: (chains, draws, trials)\n",
    "\n",
    "    # Compute mean switching probability for each trial\n",
    "    p_prior_mean = np.mean(p_prior_samples, axis=(0, 1))  # Average over chains and draws\n",
    "\n",
    "    # Analyze switching behavior by coherence bins\n",
    "    coherence_bins = np.array([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    bin_centers = (coherence_bins[:-1] + coherence_bins[1:]) / 2\n",
    "\n",
    "    print(\"Switching behavior by coherence level:\")\n",
    "    print(\"Coherence Range | Mean P(Prior) | Std P(Prior) | N Trials\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for i in range(len(coherence_bins) - 1):\n",
    "        mask = (coherence >= coherence_bins[i]) & (coherence < coherence_bins[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            p_mean = np.mean(p_prior_mean[mask])\n",
    "            p_std = np.std(p_prior_mean[mask])\n",
    "            n_trials = np.sum(mask)\n",
    "            print(f\"{coherence_bins[i]:.1f} - {coherence_bins[i+1]:.1f}     | {p_mean:.3f}      | {p_std:.3f}     | {n_trials}\")\n",
    "\n",
    "    # Analyze prediction errors by inferred strategy\n",
    "    print(\"\\nPrediction errors by inferred strategy:\")\n",
    "\n",
    "    # For each trial, determine most likely strategy\n",
    "    prior_strategy_mask = p_prior_mean > 0.5\n",
    "    likelihood_strategy_mask = ~prior_strategy_mask\n",
    "\n",
    "    # Calculate angular errors for each strategy\n",
    "    def angular_error(pred, obs):\n",
    "        return np.degrees(np.arctan2(np.sin(pred - obs), np.cos(pred - obs)))\n",
    "\n",
    "    if np.sum(prior_strategy_mask) > 0:\n",
    "        prior_errors = angular_error(theta_prior_rad[prior_strategy_mask],\n",
    "                                   theta_human_est_rad[prior_strategy_mask])\n",
    "        print(f\"Prior strategy trials (n={np.sum(prior_strategy_mask)}):\")\n",
    "        print(f\"  Mean error: {np.mean(np.abs(prior_errors)):.1f}°\")\n",
    "        print(f\"  Std error: {np.std(prior_errors):.1f}°\")\n",
    "\n",
    "    if np.sum(likelihood_strategy_mask) > 0:\n",
    "        likelihood_errors = angular_error(theta_stim_rad[likelihood_strategy_mask],\n",
    "                                        theta_human_est_rad[likelihood_strategy_mask])\n",
    "        print(f\"Likelihood strategy trials (n={np.sum(likelihood_strategy_mask)}):\")\n",
    "        print(f\"  Mean error: {np.mean(np.abs(likelihood_errors)):.1f}°\")\n",
    "        print(f\"  Std error: {np.std(likelihood_errors):.1f}°\")\n",
    "\n",
    "    # Create additional visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Detailed Switching Analysis', fontsize=16)\n",
    "\n",
    "    # 1. P(Prior) vs Coherence scatter\n",
    "    ax1 = axes[0]\n",
    "    scatter = ax1.scatter(coherence, p_prior_mean, c=coherence, cmap='viridis', alpha=0.6)\n",
    "    ax1.set_xlabel('Motion Coherence')\n",
    "    ax1.set_ylabel('P(Use Prior)')\n",
    "    ax1.set_title('Trial-by-Trial Switching Probability')\n",
    "    plt.colorbar(scatter, ax=ax1, label='Coherence')\n",
    "\n",
    "    # Add binned averages\n",
    "    for i in range(len(coherence_bins) - 1):\n",
    "        mask = (coherence >= coherence_bins[i]) & (coherence < coherence_bins[i+1])\n",
    "        if np.sum(mask) > 5:  # Only if enough trials\n",
    "            ax1.scatter(bin_centers[i], np.mean(p_prior_mean[mask]),\n",
    "                       s=100, c='red', marker='x', linewidth=3)\n",
    "\n",
    "    # 2. Strategy classification\n",
    "    ax2 = axes[1]\n",
    "    prior_coherence = coherence[prior_strategy_mask]\n",
    "    likelihood_coherence = coherence[likelihood_strategy_mask]\n",
    "\n",
    "    ax2.hist(prior_coherence, bins=20, alpha=0.7, label=f'Prior strategy (n={len(prior_coherence)})',\n",
    "             color='orange', density=True)\n",
    "    ax2.hist(likelihood_coherence, bins=20, alpha=0.7, label=f'Likelihood strategy (n={len(likelihood_coherence)})',\n",
    "             color='blue', density=True)\n",
    "    ax2.set_xlabel('Motion Coherence')\n",
    "    ax2.set_ylabel('Density')\n",
    "    ax2.set_title('Strategy Distribution by Coherence')\n",
    "    ax2.legend()\n",
    "\n",
    "    # 3. Parameter correlation\n",
    "    ax3 = axes[2]\n",
    "    ax3.scatter(alpha_samples, beta_samples, alpha=0.6)\n",
    "    ax3.set_xlabel('α (Coherence Effect)')\n",
    "    ax3.set_ylabel('β (Baseline)')\n",
    "    ax3.set_title('Parameter Correlation')\n",
    "\n",
    "    # Add correlation coefficient\n",
    "    corr_coef = np.corrcoef(alpha_samples, beta_samples)[0, 1]\n",
    "    ax3.text(0.05, 0.95, f'r = {corr_coef:.3f}', transform=ax3.transAxes,\n",
    "             bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the analysis\n",
    "if 'switching_trace' in locals():\n",
    "    print(\"Running detailed switching analysis...\")\n",
    "    analyze_switching_behavior(switching_trace, coherence, theta_human_est_rad,\n",
    "                             theta_stim_rad, theta_prior_rad)\n",
    "\n",
    "    # Run model comparison if both models are available\n",
    "    if 'trace' in locals() and 'model' in locals():\n",
    "        print(\"Running model comparison...\")\n",
    "        switching_pred, corr_switching = compare_models(trace, switching_trace, model, switching_model)\n",
    "    else:\n",
    "        print(\"Bayesian integration model not available for comparison.\")\n",
    "        print(\"Running standalone switching model analysis...\")\n",
    "\n",
    "        # Generate posterior predictive samples\n",
    "        with switching_model:\n",
    "            switching_pred = pm.sample_posterior_predictive(switching_trace, random_seed=42)\n",
    "\n",
    "        # Calculate correlation\n",
    "        theta_pred_switching = switching_pred.posterior_predictive['theta_estimate'].values\n",
    "        theta_pred_switching_mean = np.mean(theta_pred_switching, axis=(0, 1))\n",
    "\n",
    "        obs_deg = np.degrees(theta_human_est_rad)\n",
    "        pred_deg_switching = np.degrees(theta_pred_switching_mean)\n",
    "        obs_deg_wrapped = ((obs_deg + 180) % 360) - 180\n",
    "        pred_deg_switching_wrapped = ((pred_deg_switching + 180) % 360) - 180\n",
    "\n",
    "        corr_switching = np.corrcoef(obs_deg_wrapped, pred_deg_switching_wrapped)[0,1]\n",
    "        print(f\"Switching model prediction correlation: r = {corr_switching:.3f}\")\n",
    "else:\n",
    "    print(\"Switching model failed to sample successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YcK5iSmKX-3"
   },
   "source": [
    "# Hypothesis V Conclusion: Switching Observer Model Interpretation\n",
    "\n",
    "## Model Behavior and Parameter Estimates\n",
    "\n",
    "**Switching Strategy Depends on Coherence:**\n",
    "The model shows people are more likely to use their prior (i.e., ignore sensory evidence) at low coherence (32% at coherence 0.075), and less likely at high coherence (~16% at coherence 0.225).\n",
    "\n",
    "- The negative coherence effect (α ≈ -4.5) confirms that higher coherence makes people more likely to rely on sensory evidence rather than their prior\n",
    "- This suggests a reasonable adaptive strategy: when sensory information is unreliable (low coherence), fall back on prior expectations\n",
    "\n",
    "**Baseline Switching Tendency:**\n",
    "- The baseline parameter (β ≈ -0.5) indicates that even without considering coherence, people have a moderate tendency to use sensory evidence over their prior\n",
    "- This suggests the default strategy is to trust sensory information when available\n",
    "\n",
    "---\n",
    "\n",
    "## Response Precision by Strategy\n",
    "\n",
    "**Dramatic Difference in Noise Levels:**\n",
    "- When using the prior strategy: Very low noise (~0.4 radians std ≈ 23°)\n",
    "- When using the likelihood strategy: Much higher noise (~0.9 radians std ≈ 52°)\n",
    "\n",
    "This is counterintuitive! It suggests that when people \"ignore\" sensory evidence and rely on their prior, they're actually more precise in their reports. This could indicate:\n",
    "1. **Confidence-based reporting**: When people decide to use their prior, they're very confident and report precisely\n",
    "2. **Motor noise dominates likelihood strategy**: When trying to use sensory evidence, additional noise from perceptual processing or motor execution degrades precision\n",
    "\n",
    "---\n",
    "\n",
    "## Model Fit Quality\n",
    "\n",
    "**Moderate Predictive Performance:**\n",
    "- Correlation of r = 0.436 between predictions and observations\n",
    "- The prediction scatter plot shows the model captures some systematic patterns but misses considerable individual trial variability\n",
    "- Mean prediction error of 25.3° indicates systematic bias\n",
    "\n",
    "**Residual Pattern:**\n",
    "- The residuals are roughly centered but show some systematic deviations\n",
    "- The distribution has heavier tails than expected, suggesting the model doesn't fully capture the response variability\n",
    "\n",
    "---\n",
    "\n",
    "## Cognitive Interpretation\n",
    "\n",
    "**Evidence for Non-Bayesian Strategy:**\n",
    "This switching model suggests people don't optimally integrate prior and likelihood information. Instead, they appear to use a **winner-take-all** strategy where they either:\n",
    "1. Rely heavily on their prior expectation (with high precision)\n",
    "2. Rely heavily on sensory evidence (with lower precision due to additional noise)\n",
    "\n",
    "**Adaptive but Suboptimal:**\n",
    "- The coherence-dependent switching is adaptive (use prior when sensory evidence is weak)\n",
    "- But it's suboptimal compared to Bayesian integration, which would weight both sources proportionally\n",
    "- This could reflect cognitive limitations in integration processes or strategic simplification\n",
    "\n",
    "---\n",
    "\n",
    "## Key Questions for Further Investigation\n",
    "\n",
    "1. **Model Comparison**: How does this compare to the Bayesian integration model? The switching model's moderate fit suggests it may not be the complete story.\n",
    "\n",
    "2. **Precision Paradox**: Why is prior-based responding more precise? This unexpected finding warrants deeper investigation.\n",
    "\n",
    "3. **Individual Differences**: Are there consistent individual differences in switching tendencies that might explain the residual variance?\n",
    "\n",
    "The results suggest human perceptual decision-making may involve discrete strategic choices rather than continuous optimal integration, with interesting implications for theories of bounded rationality in perception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbyqLdFkKZpZ"
   },
   "source": [
    "## Acknowledgements:\n",
    "Daphne Zhang, Jacob Boulrice, Shayla Schwartz, Yi Gao, Claude Sonnet 4, GPT-4o"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
