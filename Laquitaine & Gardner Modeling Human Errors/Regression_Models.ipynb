{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nvdru8yBIB6s"
   },
   "source": [
    "**Author:** Arefeh Sherafati\n",
    "\n",
    "**Data loading author:** steeve.laquitaine@epfl.ch: Courtesy of Neuromatch Academy Computational Neuroscience Track."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLy9-aMLILcr"
   },
   "source": [
    "## Paper Summary: *A Switching Bayesian Observer Model for Human Perceptual Estimation*  \n",
    "**Paper**: [Laquitaine & Gardner, Neuron, 2018](https://doi.org/10.1016/j.neuron.2017.12.011)\n",
    "\n",
    "---\n",
    "\n",
    "### Main Objective\n",
    "To understand how **prior expectations** and **sensory evidence** interact in human perception, especially during motion direction estimation under uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### Experimental Design\n",
    "- **Subjects**: 12 human participants  \n",
    "- **Task**: Motion direction estimation  \n",
    "- **Manipulations**:\n",
    "  - **Sensory uncertainty**: Controlled via motion coherence (low coherence = noisier input)\n",
    "  - **Priors**: Directions drawn from different underlying distributions (some more frequent)\n",
    "\n",
    "- **Data**: Continuous direction estimates across ~83,000 trials\n",
    "\n",
    "---\n",
    "\n",
    "### Key Questions\n",
    "- Do humans combine prior knowledge and sensory input in a **Bayesian-optimal** way?  \n",
    "- Or is behavior better explained by **heuristics**, such as switching between prior and sensory evidence?\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Approach\n",
    "Compared three observer models:\n",
    "- **Bayesian model**: Integrates prior + likelihood into a posterior\n",
    "- **Switching model**: Chooses **either** prior **or** likelihood on each trial\n",
    "- **Weighted mixture model**: Combines both with flexible weighting\n",
    "\n",
    "Fitted models to:\n",
    "- Trial-level behavior (errors and reaction times)  \n",
    "- Used **likelihood-based model comparison** to evaluate fits\n",
    "\n",
    "---\n",
    "\n",
    "### Key Findings\n",
    "- Human observers **do not always perform Bayesian integration**.\n",
    "- Behavior is better explained by a **switching observer model**, suggesting:\n",
    "  - People flexibly rely on **prior** or **sensory** information depending on context or confidence\n",
    "- This **switching strategy** yields **more robust behavior** under uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "### Broader Implications\n",
    "- Challenges the notion that perception is always **Bayesian**  \n",
    "- Suggests perceptual inference may be **context-dependent**  \n",
    "- Introduces a **novel modeling framework** (switching observer) that better reflects human behavior\n",
    "\n",
    "---\n",
    "\n",
    "### Key Contributions\n",
    "- Introduction of a **switching observer model**  \n",
    "- Collection of a large-scale behavioral dataset (~83k trials)  \n",
    "- Empirical demonstration that switching strategy outperforms strict Bayesian integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ8LYiPcIL5K"
   },
   "outputs": [],
   "source": [
    "# @title Dependencies\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import os, requests\n",
    "from scipy.stats import norm, uniform\n",
    "from matplotlib.colors import ListedColormap\n",
    "from numpy import pi\n",
    "from copy import copy\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stTcLU2DIPTD"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "rcParams['figure.figsize'] = [20, 4]\n",
    "rcParams['font.size'] = 11\n",
    "rcParams['axes.spines.top'] = False\n",
    "rcParams['axes.spines.right'] = False\n",
    "rcParams['figure.autolayout'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tj5kxfcwIXIw"
   },
   "outputs": [],
   "source": [
    "# @title Data retrieval\n",
    "\n",
    "url = \"https://github.com/steevelaquitaine/projInference/raw/gh-pages/data/csv/data01_direction4priors.csv\"\n",
    "try:\n",
    "  RequestAPI = requests.get(url)\n",
    "except requests.ConnectionError:\n",
    "  print(\"Failed to download data. Please contact steeve.laquitaine@epfl.ch\")\n",
    "else:\n",
    "  if RequestAPI.status_code != requests.codes.ok:\n",
    "    print(\"Failed to download data. Please contact steeve.laquitaine@epfl.ch\")\n",
    "  else:\n",
    "    with open(\"data01_direction4priors.csv\", \"wb\") as fid:\n",
    "      fid.write(RequestAPI.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "KoHcXNKgJIap",
    "outputId": "0bf2e3eb-e1e1-432a-ddd9-90bbfa65bcfb"
   },
   "outputs": [],
   "source": [
    "# @title Data loading\n",
    "data = pd.read_csv(\"data01_direction4priors.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1dTx1IZInQG"
   },
   "source": [
    "### Data dictionary\n",
    "\n",
    "`data` contains sessions from 12 human subjects, data from [Laquitaine & Gardner, 2018](https://doi.org/10.1016/j.neuron.2017.12.011).\n",
    "\n",
    "Subjects had to estimate the direction of stimulus motion directions.\n",
    "\n",
    "* `data['trial_index']`: trial index\n",
    "* `data['trial_time']`: time at which trial starts with th e central fixation dot\n",
    "* `data['response_arrow_start_angle']`: the angle of the response arrow at the start of the response phase.\n",
    "* `data['motion_direction']`: the stimulus motion direction\n",
    "* `data['motion_coherence']`: the stimulus motion coherence\n",
    "* `data['estimate_x']`: x cartesian coordinate of the stimulus motion direction\n",
    "* `data['estimate_y']`: y cartesian coordinate of the stimulus motion direction\n",
    "* `data['reaction_time']`: subject's reaction time\n",
    "* `data['raw_response_time']`: subject response time since the start of the run (of about 200 trials)\n",
    "* `data['prior_std']`: It is the standard deviation of the statistical distribution (motion direction generative process over trials, which we call \"experimental prior\") from which we sampled the stimulus motion direction displayed in each trial.\n",
    "* `data['prior_mean']`: the most frequently displayed motion direction. It is the mean of the statistical distribution (motion direction generative process over trials, which we call \"experimental prior\") from which we sampled the stimulus motion direction displayed in each trial.\n",
    "* `data['subject_id']`: the id of the subject for which behavior was recorded.\n",
    "* `data['experiment_name']`: the name of the experiment. This dataaset only contains the \"data01_direction4priors\" experiment in which subject underwent a task in which four motion direction were sampled from one of four priors with 10, 40, 60 and 80 degree standard deviations in each block of about 200 trials. The mean of the \"experimental prior\"  was fixed at 225 deg.\n",
    "* `data['experiment_id']`: the id of the experiment.\n",
    "* `data['session_id']`: the id of the session.\n",
    "* `data['run_id']`: the id of the run.\n",
    "\n",
    "\n",
    "The complete original dataset is stored in .mat files here: https://data.mendeley.com/datasets/nxkvtrj9ps/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvfHnHmdIk4j"
   },
   "outputs": [],
   "source": [
    "# @title Utils\n",
    "def build_generative_model(std, mean):\n",
    "    \"\"\"create hidden generative process for motion direction\n",
    "    the experimental Gaussian prior\n",
    "    \"\"\"\n",
    "    # set direction state space\n",
    "    x = np.arange(1, 360, 1)\n",
    "\n",
    "    # calculate probability distribution\n",
    "    pdf = norm.pdf(x, loc=mean, scale=std)\n",
    "    pdf /= sum(pdf)\n",
    "    return pdf\n",
    "\n",
    "def generate_directions(mean, std, n_trials, seed):\n",
    "    return np.round(norm.rvs(loc=mean, scale=std, size=n_trials, random_state=seed))\n",
    "\n",
    "\n",
    "def learn_generative_process(motion_directions, learning_rate, x, n_trials):\n",
    "    \"\"\"learn the generative process\n",
    "    \"\"\"\n",
    "    # set subject initial belief state\n",
    "    initial_prior = uniform.pdf(x, loc=x[0], scale=x[-1])\n",
    "    initial_prior /= sum(initial_prior)\n",
    "    prior = copy.copy(initial_prior)\n",
    "    observed = np.zeros((len(x)))\n",
    "    prediction_errors = []\n",
    "    priors = []\n",
    "\n",
    "    for old_trial in range(0, n_trials):\n",
    "\n",
    "        # locate the observed state component\n",
    "        state_loc = np.where(x == int(motion_directions[old_trial]))[0][0]\n",
    "\n",
    "        # compute its state prediction error\n",
    "        state_pred_error = learning_rate * (1 - prior[state_loc])\n",
    "\n",
    "        # use the prediction error to update the state belief\n",
    "        prior[state_loc] = prior[state_loc] + state_pred_error\n",
    "        prior /= sum(prior)\n",
    "\n",
    "        # tape\n",
    "        prediction_errors.append(state_pred_error)\n",
    "        priors.append(copy.copy(prior))\n",
    "    return priors\n",
    "\n",
    "# circular statistics utils\n",
    "# -------------------\n",
    "def get_cartesian_to_deg(\n",
    "    x: np.ndarray, y: np.ndarray, signed: bool\n",
    ") -> np.ndarray:\n",
    "    \"\"\"convert cartesian coordinates to\n",
    "    angles in degree\n",
    "    Args:\n",
    "        x (np.ndarray): x coordinate\n",
    "        y (np.ndarray): y coordinate\n",
    "        signed (boolean): True (signed) or False (unsigned)\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_cartesian_to_deg\n",
    "            x = np.array([1, 0, -1, 0])\n",
    "            y = np.array([0, 1, 0, -1])\n",
    "            degree = get_cartesian_to_deg(x,y,False)\n",
    "            # Out: array([  0.,  90., 180., 270.])\n",
    "    Returns:\n",
    "        np.ndarray: angles in degree\n",
    "    \"\"\"\n",
    "    # convert to radian (ignoring divide by 0 warning)\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        degree = np.arctan(y / x)\n",
    "\n",
    "    # convert to degree and adjust based\n",
    "    # on quadrant\n",
    "    for ix in range(len(x)):\n",
    "        if (x[ix] >= 0) and (y[ix] >= 0):\n",
    "            degree[ix] = degree[ix] * 180 / np.pi\n",
    "        elif (x[ix] == 0) and (y[ix] == 0):\n",
    "            degree[ix] = 0\n",
    "        elif x[ix] < 0:\n",
    "            degree[ix] = degree[ix] * 180 / np.pi + 180\n",
    "        elif (x[ix] >= 0) and (y[ix] < 0):\n",
    "            degree[ix] = degree[ix] * 180 / np.pi + 360\n",
    "\n",
    "    # if needed, convert signed to unsigned\n",
    "    if not signed:\n",
    "        degree[degree < 0] = degree[degree < 0] + 360\n",
    "    return degree\n",
    "\n",
    "def get_deg_to_rad(deg: np.array, signed: bool):\n",
    "    \"\"\"convert angles in degree to radian\n",
    "    Args:\n",
    "        deg (np.array): angles in degree\n",
    "        signed (bool): True (signed) or False (unsigned)\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_deg_to_rad\n",
    "            radians = get_deg_to_rad(np.array([0, 90, 180, 270], True)\n",
    "            Out: array([ 0., 1.57079633, 3.14159265, -1.57079633])\n",
    "    Returns:\n",
    "        np.ndarray: angles in radian\n",
    "    \"\"\"\n",
    "    # get unsigned radians (1:2*pi)\n",
    "    rad = (deg / 360) * 2 * pi\n",
    "\n",
    "    # get signed radians(-pi:pi)\n",
    "    if signed:\n",
    "        rad[deg > 180] = (deg[deg > 180] - 360) * (\n",
    "            2 * pi / 360\n",
    "        )\n",
    "    return rad\n",
    "\n",
    "def get_polar_to_cartesian(\n",
    "    angle: np.ndarray, radius: float, type: str\n",
    ") -> dict:\n",
    "    \"\"\"convert angle in degree or radian to cartesian coordinates\n",
    "    Args:\n",
    "        angle (np.ndarray): angles in degree or radian\n",
    "        radius (float): radius\n",
    "        type (str): \"polar\" or \"radian\"\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_polar_to_cartesian\n",
    "            degree = np.array([0, 90, 180, 270])\n",
    "            cartesian = get_polar_to_cartesian(degree, 1, \"polar\")\n",
    "            cartesian.keys()\n",
    "\n",
    "            # Out: dict_keys(['deg', 'rad', 'cart'])\n",
    "\n",
    "            cartesian[\"cart\"]\n",
    "\n",
    "            # Out: array([[ 1.,  0.],\n",
    "            #            [ 0.,  1.],\n",
    "            #            [-1.,  0.],\n",
    "            #            [-0., -1.]])\n",
    "    Returns:\n",
    "        dict: _description_\n",
    "    \"\"\"\n",
    "    # convert to radian if needed\n",
    "    theta = dict()\n",
    "    if type == \"polar\":\n",
    "        theta[\"deg\"] = angle\n",
    "        theta[\"rad\"] = angle * np.pi / 180\n",
    "    elif type == \"radian\":\n",
    "        theta[\"deg\"] = get_deg_to_rad(angle, False)\n",
    "        theta[\"rad\"] = angle\n",
    "\n",
    "    # convert to cartesian coordinates\n",
    "    x = radius * np.cos(theta[\"rad\"])\n",
    "    y = radius * np.sin(theta[\"rad\"])\n",
    "\n",
    "    # round to 10e-4\n",
    "    x = np.round(x, 4)\n",
    "    y = np.round(y, 4)\n",
    "\n",
    "    # reshape as (N angles x 2 coord)\n",
    "    theta[\"cart\"] = np.vstack([x, y]).T\n",
    "    return theta\n",
    "\n",
    "def get_circ_weighted_mean_std(\n",
    "    angle: np.ndarray, proba: np.ndarray, type: str\n",
    ") -> dict:\n",
    "    \"\"\"calculate circular data statistics\n",
    "    Args:\n",
    "        angle (np.ndarray): angles in degree or cartesian coordinates\n",
    "        proba (np.ndarray): each angle's probability of occurrence\n",
    "        type (str): \"polar\" or \"cartesian\"\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            import numpy as np\n",
    "            from bsfit.nodes.cirpy.utils import get_circ_weighted_mean_std\n",
    "            degree = np.array([358, 0, 2, 88, 90, 92])\n",
    "            proba = np.array([1, 1, 1, 1, 1, 1])/6\n",
    "            output = get_circ_weighted_mean_std(degree, proba, \"polar\")\n",
    "            output.keys()\n",
    "            # Out: dict_keys(['coord_all', 'deg_all', 'coord_mean', 'deg_mean',\n",
    "            #               'deg_all_for_std', 'deg_mean_for_std', 'deg_var',\n",
    "            #               'deg_std', 'deg_sem'])\n",
    "            output[\"deg_mean\"]\n",
    "            # Out: array([45.])\n",
    "            output[\"deg_std\"]\n",
    "            # array([45.02961988])\n",
    "    Returns:\n",
    "        (dict): angle summary statistics (mean, std, var, sem)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: type is not \"polar\" or \"cartesian\"\n",
    "    \"\"\"\n",
    "\n",
    "    angle = angle.copy()\n",
    "\n",
    "    # if polar, convert to cartesian\n",
    "    if type == \"polar\":\n",
    "        radius = 1\n",
    "        coord = get_polar_to_cartesian(\n",
    "            angle, radius=radius, type=\"polar\"\n",
    "        )\n",
    "    elif type == \"cartesian\":\n",
    "        coord = angle\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"\"\" \"type\" can either be \"polar\" or \"cartesian\" value \"\"\"\n",
    "        )\n",
    "\n",
    "    # store angles\n",
    "    data = dict()\n",
    "    data[\"coord_all\"] = coord[\"cart\"]\n",
    "    data[\"deg_all\"] = coord[\"deg\"]\n",
    "\n",
    "    # calculate mean\n",
    "    # ..............\n",
    "    proba_for_mean = np.tile(proba[:, None], 2)\n",
    "    data[\"coord_mean\"] = np.sum(\n",
    "        proba_for_mean * data[\"coord_all\"], 0\n",
    "    )\n",
    "    data[\"coord_mean\"] = data[\"coord_mean\"][:, None]\n",
    "    data[\"deg_mean\"] = get_cartesian_to_deg(\n",
    "        data[\"coord_mean\"][0],\n",
    "        data[\"coord_mean\"][1],\n",
    "        signed=False,\n",
    "    )\n",
    "\n",
    "    # calculate std\n",
    "    # ..............\n",
    "    n_data = len(data[\"deg_all\"])\n",
    "    data[\"deg_all_for_std\"] = data[\"deg_all\"]\n",
    "    data[\"deg_mean_for_std\"] = np.tile(\n",
    "        data[\"deg_mean\"], n_data\n",
    "    )\n",
    "\n",
    "    # apply corrections\n",
    "    # when 0 <= mean <= 180\n",
    "    if data[\"deg_mean\"] + 180 <= 360:\n",
    "        for ix in range(n_data):\n",
    "            if (\n",
    "                data[\"deg_all\"][ix]\n",
    "                >= data[\"deg_mean\"] + 180\n",
    "            ):\n",
    "                data[\"deg_all_for_std\"][ix] = (\n",
    "                    data[\"deg_all\"][ix] - 360\n",
    "                )\n",
    "    else:\n",
    "        # when 180 <= mean <= 360\n",
    "        for ix in range(n_data):\n",
    "            if (\n",
    "                data[\"deg_all\"][ix]\n",
    "                <= data[\"deg_mean\"] - 180\n",
    "            ):\n",
    "                data[\"deg_mean_for_std\"][ix] = (\n",
    "                    data[\"deg_mean\"] - 360\n",
    "                )\n",
    "\n",
    "    # calculate variance, standard deviation and\n",
    "    # standard error to the mean\n",
    "    data[\"deg_var\"] = np.array(\n",
    "        [\n",
    "            sum(\n",
    "                proba\n",
    "                * (\n",
    "                    data[\"deg_all_for_std\"]\n",
    "                    - data[\"deg_mean_for_std\"]\n",
    "                )\n",
    "                ** 2\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    data[\"deg_std\"] = np.sqrt(data[\"deg_var\"])\n",
    "    data[\"deg_sem\"] = data[\"deg_std\"] / np.sqrt(n_data)\n",
    "    return data\n",
    "\n",
    "def get_signed_angle(\n",
    "    origin: np.ndarray, destination: np.ndarray, type: str\n",
    "):\n",
    "    \"\"\"get the signed angle difference between origin and destination angles\n",
    "    Args:\n",
    "        origin (np.ndarray): origin angle\n",
    "        destination (np.ndarray): destination angle\n",
    "        type (str): angle type (\"polar\", \"radian\", \"cartesian\")\n",
    "    Usage:\n",
    "        .. code-block:: python\n",
    "            angle = get_signed_angle(90, 45, 'polar')\n",
    "\n",
    "            # Out: array([45.])\n",
    "\n",
    "            angle = get_signed_angle(90, 45, 'radian')\n",
    "            # Out: array([58.3103779])\n",
    "            origin = np.array([[0, 1]])\n",
    "            destination = np.array([[1, 0]])\n",
    "            angle = get_signed_angle(origin, destination, \"cartesian\")\n",
    "\n",
    "            # Out: array([90.])\n",
    "    Returns:\n",
    "        (np.ndarray): signed angle differences\n",
    "    \"\"\"\n",
    "\n",
    "    # convert to cartesian coordinates\n",
    "    if type == \"polar\" or type == \"radian\":\n",
    "        origin_dict = get_polar_to_cartesian(\n",
    "            origin, radius=1, type=type\n",
    "        )\n",
    "        destination_dict = get_polar_to_cartesian(\n",
    "            destination, radius=1, type=type\n",
    "        )\n",
    "    elif type == \"cartesian\":\n",
    "        origin_dict = dict()\n",
    "        destination_dict = dict()\n",
    "        origin_dict[\"cart\"] = origin\n",
    "        destination_dict[\"cart\"] = destination\n",
    "\n",
    "    # get coordinates\n",
    "    xV1 = origin_dict[\"cart\"][:, 0]\n",
    "    yV1 = origin_dict[\"cart\"][:, 1]\n",
    "    xV2 = destination_dict[\"cart\"][:, 0]\n",
    "    yV2 = destination_dict[\"cart\"][:, 1]\n",
    "\n",
    "    # Calculate the angle separating the\n",
    "    # two vectors in degrees\n",
    "    angle = -(180 / np.pi) * np.arctan2(\n",
    "        xV1 * yV2 - yV1 * xV2, xV1 * xV2 + yV1 * yV2\n",
    "    )\n",
    "    return angle\n",
    "\n",
    "def get_combination_set(database: np.ndarray):\n",
    "    \"\"\"get the set of row combinations\n",
    "\n",
    "    Args:\n",
    "        database (np.ndarray): an N-D array\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray, np.ndarray): `combs` is the set\n",
    "        of combinations, `b` are the row indices for each combination\n",
    "        in database, `c` are the rows indices for each combination in\n",
    "        combs.\n",
    "    \"\"\"\n",
    "    combs, ia, ic = np.unique(\n",
    "        database,\n",
    "        return_index=True,\n",
    "        return_inverse=True,\n",
    "        axis=0,\n",
    "    )\n",
    "    return (combs, ia, ic)\n",
    "\n",
    "def get_data_stats(data: pd.Series, output: dict):\n",
    "    \"\"\"calculate data statistics\n",
    "\n",
    "    Args:\n",
    "        data (pd.Series): stimulus feature estimates\n",
    "        output (dict): ::\n",
    "\n",
    "            'PestimateGivenModel': estimate probabilities\n",
    "            'map': max-a-posteriori percepts\n",
    "            'conditions': task conditions\n",
    "\n",
    "    Returns:\n",
    "        (dict): returns data mean and std to output\n",
    "    \"\"\"\n",
    "    # get conditions\n",
    "    cond = output[\"conditions\"]\n",
    "\n",
    "    # initialise statistics\n",
    "    data_mean = []\n",
    "    data_std = []\n",
    "\n",
    "    # get set of conditions\n",
    "    cond_set, ix, _ = get_combination_set(cond)\n",
    "\n",
    "    # record stats by condition\n",
    "    for c_i in range(len(cond_set)):\n",
    "\n",
    "        # find condition's instances\n",
    "        loc_1 = cond[:, 0] == cond_set[c_i, 0]\n",
    "        loc_2 = cond[:, 1] == cond_set[c_i, 1]\n",
    "        loc_3 = cond[:, 2] == cond_set[c_i, 2]\n",
    "\n",
    "        # get associated data\n",
    "        data_c_i = data.values[loc_1 & loc_2 & loc_3]\n",
    "\n",
    "        # set each instance with equal probability\n",
    "        trial_proba = np.tile(\n",
    "            1 / len(data_c_i), len(data_c_i)\n",
    "        )\n",
    "\n",
    "        # get statistics\n",
    "        stats = get_circ_weighted_mean_std(\n",
    "            data_c_i, trial_proba, type=\"polar\",\n",
    "        )\n",
    "\n",
    "        # record statistics\n",
    "        data_mean.append(stats[\"deg_mean\"])\n",
    "        data_std.append(stats[\"deg_std\"])\n",
    "\n",
    "    # record statistics\n",
    "    output[\"data_mean\"] = np.array(data_mean)\n",
    "    output[\"data_std\"] = np.array(data_std)\n",
    "\n",
    "    # record their condition\n",
    "    output[\"conditions\"] = cond_set\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFVE05f0Isfr"
   },
   "source": [
    "## Hypothesis I: A Basic Linear Regression Model of Circular Distance Prediction\n",
    "\n",
    "**Goal**  \n",
    "Calculate the circular distances between past trial estimates and current trial displayed directions.  \n",
    "- Split the data into train and test sets.  \n",
    "- Train a linear regression to predict the error from that feature.  \n",
    "- Report R² on the test set.  \n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- **Circular distances** are computed modulo 360 (wrapped angular differences).\n",
    "- The **prediction target** is the **angular error**, defined as:\n",
    "\n",
    "$$\n",
    "\\text{angular error} = \\text{estimate} - \\text{stimulus direction}\n",
    "$$\n",
    "\n",
    "  (wrapped within [–180°, +180°] or [0°, 360°], depending on convention)\n",
    "\n",
    "- The main **feature** is the **circular distance between the previous trial's estimate and the current trial’s stimulus direction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX3vqyMSIv30",
    "outputId": "b5bfd610-87c1-4b2d-e767-1ebea4a3604f"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# --- Step 1: Prepare Data ---\n",
    "df = data.copy()\n",
    "\n",
    "# Use existing function from notebook to convert (x, y) to degrees\n",
    "df[\"estimate_deg\"] = get_cartesian_to_deg(df[\"estimate_x\"].values, df[\"estimate_y\"].values, signed=False)\n",
    "\n",
    "# Sort to maintain temporal trial order within each subject\n",
    "df = df.sort_values([\"subject_id\", \"session_id\", \"run_id\", \"trial_index\"])\n",
    "\n",
    "# Create feature: circular distance between previous estimate and current stimulus\n",
    "df[\"prev_estimate\"] = df.groupby(\"subject_id\")[\"estimate_deg\"].shift(1)\n",
    "\n",
    "# Use built-in helper to compute circular distances\n",
    "def circular_distance_deg(a, b):\n",
    "    return np.angle(np.exp(1j * np.deg2rad(a - b)), deg=True)\n",
    "\n",
    "df[\"circ_dist_prev_est_to_stim\"] = circular_distance_deg(df[\"motion_direction\"], df[\"prev_estimate\"])\n",
    "\n",
    "# Create target: circular error = estimate - stimulus direction (wrapped)\n",
    "df[\"circular_error\"] = circular_distance_deg(df[\"estimate_deg\"], df[\"motion_direction\"])\n",
    "\n",
    "# Drop NaNs\n",
    "df = df.dropna(subset=[\"circ_dist_prev_est_to_stim\", \"circular_error\"])\n",
    "\n",
    "# --- Step 2: Split into train/test ---\n",
    "X = df[[\"circ_dist_prev_est_to_stim\"]].values\n",
    "y = df[\"circular_error\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Step 3: Train linear regression ---\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# --- Step 4: Report R² score ---\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² score on test set: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTTT_VweIyBk"
   },
   "source": [
    "### Scatter Plot with Line of Best Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "BbCKVUATI5z-",
    "outputId": "7a546545-69d0-4d4e-ba45-1796c889e543"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_test, y_test, alpha=0.3, label=\"True\")\n",
    "plt.scatter(X_test, y_pred, color='red', alpha=0.3, label=\"Predicted\")\n",
    "plt.xlabel(\"Circular Distance: Prev Estimate to Stimulus (deg)\")\n",
    "plt.ylabel(\"Angular Error (deg)\")\n",
    "plt.title(\"Prediction of Angular Error from Circular Distance\")\n",
    "plt.legend()\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo_zKExTJSpB"
   },
   "source": [
    "### Binned Prediction Plot (Mean ± Std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 628
    },
    "id": "Fm2EG11GJRUV",
    "outputId": "6ea27437-c68d-4ed9-8826-9709efad8a42"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Bin the X values (e.g. into 20 bins)\n",
    "df_plot = pd.DataFrame({\n",
    "    \"feature\": X_test.flatten(),\n",
    "    \"true\": y_test,\n",
    "    \"pred\": y_pred\n",
    "})\n",
    "\n",
    "bins = np.linspace(-180, 180, 20)\n",
    "df_plot[\"bin\"] = pd.cut(df_plot[\"feature\"], bins)\n",
    "\n",
    "# Compute mean and std per bin\n",
    "mean_true = df_plot.groupby(\"bin\")[\"true\"].mean()\n",
    "mean_pred = df_plot.groupby(\"bin\")[\"pred\"].mean()\n",
    "std_true = df_plot.groupby(\"bin\")[\"true\"].std()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.errorbar(mean_true.index.categories.mid, mean_true, yerr=std_true, fmt='o', label=\"Mean True ± STD\")\n",
    "plt.plot(mean_pred.index.categories.mid, mean_pred, 'r--', label=\"Mean Predicted\")\n",
    "plt.xlabel(\"Binned Circular Distance (deg)\")\n",
    "plt.ylabel(\"Angular Error (deg)\")\n",
    "plt.title(\"Binned Fit of Angular Error vs Circular Distance\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NA8qZu5YJctS"
   },
   "source": [
    "## Hypothesis I Auxiliary Predictive Test: Effect of Absolute Circular Distance on Estimation Error\n",
    "\n",
    "In our original model, we used the **signed circular distance** between the **previous trial’s estimate** and the **current trial’s stimulus direction** as the predictor. This assumes that the **direction** of the difference (clockwise vs counterclockwise) influences behavior.\n",
    "\n",
    "However, we now hypothesize that **only the magnitude** of the difference matters — not the direction. That is, larger angular separations between past and current stimuli might lead to **grea**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "eSP0nvReJY9i",
    "outputId": "2789b9b7-8608-45d0-e765-51b9a3f36617"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Use absolute value of circular distance as the feature ---\n",
    "df[\"abs_circ_dist\"] = np.abs(df[\"circ_dist_prev_est_to_stim\"])\n",
    "\n",
    "# Drop NaNs\n",
    "df_model = df.dropna(subset=[\"abs_circ_dist\", \"circular_error\"])\n",
    "\n",
    "# Prepare data\n",
    "X = df_model[[\"abs_circ_dist\"]].values\n",
    "y = df_model[\"circular_error\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit linear regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Report R²\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"R² score using absolute circular distance: {r2:.3f}\")\n",
    "\n",
    "# Plot using your specified format\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_test, y_test, alpha=0.3, label=\"True\")\n",
    "plt.scatter(X_test, y_pred, color='red', alpha=0.3, label=\"Predicted\")\n",
    "plt.xlabel(\"Circular Distance: Prev Estimate to Stimulus (deg)\")  # Keep original axis label\n",
    "plt.ylabel(\"Angular Error (deg)\")\n",
    "plt.title(\"Prediction of Angular Error from Circular Distance\")\n",
    "plt.legend()\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mECJx7l7JV9O"
   },
   "source": [
    "## Hypothesis I Conclusion: Predictive Power of Circular Distance Features\n",
    "\n",
    "We tested whether the **circular distance between the previous trial’s estimate and the current trial’s stimulus direction** could predict the **angular error** in human motion direction estimation.\n",
    "\n",
    "Two models were compared:\n",
    "- One using the **signed circular distance** as a feature.\n",
    "- One using the **absolute circular distance** as a feature.\n",
    "---\n",
    "### Results\n",
    "- The **signed circular distance** model yielded an R² ≈ 0.01\n",
    "- The **absolute circular distance** model yielded an R² ≈ 0.000\n",
    "---\n",
    "### Interpretation\n",
    "Both models explained **very little to no variance** in the estimation error, suggesting that:\n",
    "- The circular distance between past and current trials alone is **not a strong linear predictor** of angular error.\n",
    "- The relationship may be **nonlinear**, **subject-specific**, or influenced by **additional factors** such as:\n",
    "  - Sensory uncertainty (e.g., motion coherence)\n",
    "  - Prior distribution strength (`prior_std`)\n",
    "  - Decision confidence or reaction time\n",
    "  - Trial history beyond just the immediately previous trial\n",
    "---\n",
    "### Next Steps\n",
    "To better understand human estimation behavior, future models should:\n",
    "- Include **multiple features** in a multivariate regression\n",
    "- Explore **nonlinear models**\n",
    "- Consider subject-specific or hierarchical modeling\n",
    "- Potentially use **Bayesian observer frameworks** that align with prior work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIA4mVJWJhQa"
   },
   "source": [
    "## Hypothesis II: Multivariate Predictors of Estimation Error\n",
    "\n",
    "Based on previous results, we observed that the circular distance between the previous trial’s estimate and the current stimulus direction — whether signed or absolute — is **not a strong standalone predictor** of human angular error in this task.\n",
    "\n",
    "We now hypothesize that:\n",
    "> **A combination of multiple task-relevant features** can better predict the estimation error than any single feature alone.\n",
    "\n",
    "### Features Included:\n",
    "- `abs_circ_dist`: Absolute circular distance between previous estimate and current stimulus\n",
    "- `motion_coherence`: Strength of sensory evidence (higher = less uncertainty)\n",
    "- `prior_std`: Variability in the experimental prior (lower = stronger prior)\n",
    "- `reaction_time`: May reflect decision confidence or task difficulty\n",
    "\n",
    "We will fit a **multiple linear regression model** using these features and evaluate its performance using R² on a held-out test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "88ZQ2WSuJUZj",
    "outputId": "2dffdd7d-98e1-400b-d662-4575d29be11c"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Define features ---\n",
    "features = [\"abs_circ_dist\", \"motion_coherence\", \"prior_std\", \"reaction_time\"]\n",
    "df_model_multi = df.dropna(subset=features + [\"circular_error\"])\n",
    "\n",
    "print(f\"Sample size: {len(df_model_multi)} observations\")\n",
    "print(f\"Missing data removed: {len(df) - len(df_model_multi)} observations\")\n",
    "\n",
    "# Prepare data\n",
    "X = df_model_multi[features].values\n",
    "y = df_model_multi[\"circular_error\"].values\n",
    "\n",
    "# Standardize features for better coefficient interpretation\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "baseline_rmse = np.std(y_test)  # RMSE of always predicting mean\n",
    "\n",
    "print(f\"\\nModel Performance:\")\n",
    "print(f\"R² score: {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"Baseline RMSE (predicting mean): {baseline_rmse:.3f}\")\n",
    "print(f\"RMSE improvement: {((baseline_rmse - rmse) / baseline_rmse * 100):.1f}%\")\n",
    "\n",
    "# Statistical significance (rough approximation)\n",
    "n = len(y_test)\n",
    "if r2 > 0:\n",
    "    f_stat = (r2 / (1 - r2)) * ((n - len(features) - 1) / len(features))\n",
    "    print(f\"Approximate F-statistic: {f_stat:.3f}\")\n",
    "\n",
    "# --- Enhanced Plot 1: Predicted vs. True values ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.4, s=20)\n",
    "plt.xlabel(\"True Angular Error (deg)\")\n",
    "plt.ylabel(\"Predicted Angular Error (deg)\")\n",
    "plt.title(f\"Predicted vs. True Angular Error (R² = {r2:.3f})\")\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(y_test, y_pred, 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(y_test, p(y_test), \"b--\", alpha=0.8, label=f'Actual Trend (slope={z[0]:.3f})')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Residuals vs. Predicted ---\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_pred, residuals, alpha=0.4, s=20)\n",
    "plt.axhline(0, color='red', linestyle='--', linewidth=2, label='Perfect Predictions')\n",
    "plt.xlabel(\"Predicted Angular Error (deg)\")\n",
    "plt.ylabel(\"Residuals (True - Predicted)\")\n",
    "plt.title(\"Residuals vs. Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add LOESS smooth line to detect patterns\n",
    "from scipy import stats\n",
    "if len(y_pred) > 50:  # Only if enough points\n",
    "    # Sort by predicted values for smooth line\n",
    "    sort_idx = np.argsort(y_pred)\n",
    "    y_pred_sorted = y_pred[sort_idx]\n",
    "    residuals_sorted = residuals[sort_idx]\n",
    "\n",
    "    # Simple moving average as LOESS approximation\n",
    "    window = max(20, len(y_pred) // 20)\n",
    "    smoothed = pd.Series(residuals_sorted).rolling(window=window, center=True).mean()\n",
    "    plt.plot(y_pred_sorted, smoothed, 'orange', linewidth=2, label='Trend')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: Standardized Feature Coefficients ---\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Coefficient\": lr.coef_,\n",
    "    \"Abs_Coefficient\": np.abs(lr.coef_)\n",
    "}).sort_values(\"Abs_Coefficient\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red' if x < 0 else 'blue' for x in coef_df[\"Coefficient\"]]\n",
    "plt.barh(coef_df[\"Feature\"], coef_df[\"Coefficient\"], color=colors, alpha=0.7)\n",
    "plt.xlabel(\"Standardized Coefficient\")\n",
    "plt.title(\"Feature Importance (Standardized Coefficients)\")\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Analysis: Feature Correlations ---\n",
    "plt.figure(figsize=(8, 6))\n",
    "feature_corr = df_model_multi[features + [\"circular_error\"]].corr()\n",
    "sns.heatmap(feature_corr, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f')\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Statistics ---\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(df_model_multi[features].describe())\n",
    "\n",
    "print(f\"\\nFeature-Target Correlations:\")\n",
    "for feature in features:\n",
    "    corr = df_model_multi[feature].corr(df_model_multi[\"circular_error\"])\n",
    "    print(f\"{feature}: {corr:.3f}\")\n",
    "\n",
    "# --- Diagnostic: Check for outliers ---\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i, feature in enumerate(features):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.boxplot(df_model_multi[feature])\n",
    "    plt.title(f\"{feature}\")\n",
    "    plt.xticks([])\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Feature Distributions (Check for Outliers)\", y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# --- Interpretation Summary ---\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"INTERPRETATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"• Model explains {r2*100:.1f}% of variance in angular error\")\n",
    "print(f\"• RMSE is {rmse:.1f}°, vs baseline of {baseline_rmse:.1f}°\")\n",
    "\n",
    "if r2 < 0.05:\n",
    "    print(\"• POOR MODEL FIT: Linear combination of these features\")\n",
    "    print(\"  does not effectively predict estimation error\")\n",
    "    print(\"• Possible reasons:\")\n",
    "    print(\"  - Non-linear relationships\")\n",
    "    print(\"  - Missing important predictors\")\n",
    "    print(\"  - High individual variability\")\n",
    "    print(\"  - Measurement noise dominates signal\")\n",
    "\n",
    "strongest_predictor = coef_df.loc[coef_df[\"Abs_Coefficient\"].idxmax(), \"Feature\"]\n",
    "print(f\"• Strongest predictor: {strongest_predictor}\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "print(f\"• Consider non-linear models (polynomial, tree-based)\")\n",
    "print(f\"• Include additional features (individual differences, trial history)\")\n",
    "print(f\"• Check for interaction effects between features\")\n",
    "print(f\"• Consider mixed-effects models to account for individual differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_0qxrwjJl8x"
   },
   "source": [
    "#  Hypothesis II Conclusion:\n",
    "\n",
    "Model Performance\n",
    "\n",
    "The extremely poor performance (R² = 0.001, RMSE = 50.182) indicates that this linear combination of features explains virtually none of the variance in angular estimation error. This suggests several important findings:\n",
    "\n",
    "Linear relationships are insufficient: The features may have non-linear relationships with estimation error, or the relationships may be more complex than a simple additive model can capture.\n",
    "Missing key predictors: The most important factors driving estimation error may not be included in your current feature set.\n",
    "High individual variability: Human estimation error in this task may be dominated by factors not captured in these variables (e.g., attention, fatigue, individual differences in strategy).\n",
    "\n",
    "Feature Effects (from the coefficient plot)\n",
    "\n",
    "Motion coherence has the largest positive coefficient (around 3.5) suggesting higher coherence is associated with larger errors (counterintuitive)\n",
    "Absolute circular distance has a moderate positive effect (around 0.8)\n",
    "Reaction time has a smaller positive effect (around 0.7)\n",
    "Prior standard deviation has minimal effect (near zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gh1IjqnIJof6"
   },
   "source": [
    "# Hypothesis III: Bayesian Linear Regression for Estimation Error Prediction\n",
    "\n",
    "Our earlier linear models provided point estimates for feature weights but could not express uncertainty in those estimates. We now hypothesize that:\n",
    "\n",
    "> A **Bayesian linear regression model** can quantify the **uncertainty in feature relationships**, especially for circular distance and other task-related variables.\n",
    "---\n",
    "### Why Bayesian Regression?\n",
    "- It assumes a **prior distribution over the weights** (e.g., Gaussian prior on coefficients).\n",
    "- It returns **posterior distributions**, not just point estimates.\n",
    "- This allows us to measure **confidence** in whether each predictor contributes meaningfully to the prediction.\n",
    "---\n",
    "### Assumptions:\n",
    "We assume:\n",
    "- Angular error is normally distributed around a linear combination of features.\n",
    "- Feature weights come from a zero-mean Gaussian prior.\n",
    "\n",
    "We will use **Bayesian Ridge Regression** from `scikit-learn`, which approximates the Bayesian posterior and provides credible intervals for coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ZR4xwJNTJlXv",
    "outputId": "e9e9784c-1bd5-4a57-e919-c8a0b63643fd"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# --- Features & target ---\n",
    "features = [\"abs_circ_dist\", \"motion_coherence\", \"prior_std\", \"reaction_time\"]\n",
    "df_model_bayes = df.dropna(subset=features + [\"circular_error\"])\n",
    "\n",
    "X = df_model_bayes[features].values\n",
    "y = df_model_bayes[\"circular_error\"].values\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- Fit Bayesian Ridge Regression ---\n",
    "bayes_model = BayesianRidge(compute_score=True)\n",
    "bayes_model.fit(X_train, y_train)\n",
    "y_pred = bayes_model.predict(X_test)\n",
    "\n",
    "# --- Evaluation ---\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"R² (Bayesian Ridge): {r2:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "# --- Extract coefficient statistics ---\n",
    "# Get posterior mean of coefficients\n",
    "coef_mean = bayes_model.coef_\n",
    "\n",
    "# Calculate posterior covariance and standard deviations\n",
    "# For Bayesian Ridge, we can estimate the uncertainty using the posterior covariance\n",
    "# Sigma = (alpha * I + beta * X.T @ X)^(-1) where alpha and beta are learned hyperparameters\n",
    "X_train_centered = X_train - np.mean(X_train, axis=0)\n",
    "precision_matrix = bayes_model.alpha_ * np.eye(X_train.shape[1]) + bayes_model.lambda_ * X_train_centered.T @ X_train_centered\n",
    "covariance_matrix = np.linalg.inv(precision_matrix)\n",
    "coef_std = np.sqrt(np.diag(covariance_matrix))\n",
    "\n",
    "# Create DataFrame for coefficients\n",
    "coef_df = pd.DataFrame({\n",
    "    \"Feature\": features,\n",
    "    \"Mean\": coef_mean,\n",
    "    \"StdDev\": coef_std\n",
    "})\n",
    "\n",
    "print(\"\\nCoefficient Statistics:\")\n",
    "print(coef_df)\n",
    "\n",
    "# --- Plot 1: Predictions vs. True values ---\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.xlabel(\"True Angular Error (deg)\")\n",
    "plt.ylabel(\"Predicted Angular Error (deg)\")\n",
    "plt.title(\"Bayesian Ridge: Predicted vs. True Angular Error\")\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "# Add diagonal line for perfect predictions\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 2: Residuals vs. Predicted ---\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(y_pred, residuals, alpha=0.3)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Angular Error (deg)\")\n",
    "plt.ylabel(\"Residuals (True - Predicted)\")\n",
    "plt.title(\"Residuals vs. Predicted (Bayesian Ridge)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot 3: Coefficients with uncertainty using matplotlib ---\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Horizontal bar plot with error bars\n",
    "plt.barh(\n",
    "    y=coef_df[\"Feature\"],\n",
    "    width=coef_df[\"Mean\"],\n",
    "    xerr=coef_df[\"StdDev\"],\n",
    "    align=\"center\",\n",
    "    alpha=0.7,\n",
    "    color=\"skyblue\",\n",
    "    ecolor=\"black\",\n",
    "    capsize=5\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Posterior Mean Coefficient\")\n",
    "plt.title(\"Posterior Mean and Uncertainty (Bayesian Ridge)\")\n",
    "plt.axvline(0, color='gray', linestyle='--')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Additional: Plot coefficient credible intervals ---\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Calculate 95% credible intervals (assuming normal posterior)\n",
    "ci_lower = coef_df[\"Mean\"] - 1.96 * coef_df[\"StdDev\"]\n",
    "ci_upper = coef_df[\"Mean\"] + 1.96 * coef_df[\"StdDev\"]\n",
    "\n",
    "# Create error bar plot\n",
    "plt.errorbar(\n",
    "    x=coef_df[\"Mean\"],\n",
    "    y=range(len(features)),\n",
    "    xerr=1.96 * coef_df[\"StdDev\"],\n",
    "    fmt='o',\n",
    "    capsize=5,\n",
    "    capthick=2,\n",
    "    markersize=8\n",
    ")\n",
    "\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.ylabel(\"Features\")\n",
    "plt.title(\"Bayesian Ridge Coefficients with 95% Credible Intervals\")\n",
    "plt.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print model hyperparameters ---\n",
    "print(f\"\\nLearned Hyperparameters:\")\n",
    "print(f\"Alpha (precision of noise): {bayes_model.alpha_:.6f}\")\n",
    "print(f\"Lambda (precision of coefficients): {bayes_model.lambda_:.6f}\")\n",
    "print(f\"Log marginal likelihood: {bayes_model.scores_[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "creJXgz0J3fW"
   },
   "source": [
    "# Hypothesis III Conclusion: Bayesian Linear Regression Interpretation\n",
    "\n",
    "- **R² score**: 0.001  \n",
    "- **RMSE**: ~50.19 degrees\n",
    "\n",
    "These results indicate that the model explained **virtually no variance** in the angular error. The RMSE confirms that the predictions are highly inaccurate on average.\n",
    "\n",
    "---\n",
    "#### 1. Predicted vs. True Angular Error:\n",
    "- The predicted values are tightly clustered around 0.\n",
    "- True values span the full range of angular error.\n",
    "- The model is clearly **underfitting** and fails to capture the signal in the data.\n",
    "---\n",
    "#### 2. Posterior Coefficient Estimates:\n",
    "- Most coefficients are close to zero with **large uncertainty bounds**.\n",
    "- `prior_std` shows a slightly positive coefficient, suggesting a **minor effect** on error, consistent with prior knowledge.\n",
    "- Overall, the model indicates **low confidence** in any single feature being predictive.\n",
    "---\n",
    "### Conclusion:\n",
    "\n",
    "- Angular error is **not linearly predictable** using these features.\n",
    "- The Bayesian framework gives useful uncertainty estimates, which reveal weak or unreliable feature effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MItzWoFIYWNP"
   },
   "source": [
    "### Export requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJzHFMU6NJ0-"
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ABzPKFWwNQR0",
    "outputId": "beceaad8-a0f1-423c-9fa0-d144ffebd3a1"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"requirements.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tULxw9j5aD5N"
   },
   "source": [
    "### Fix metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "81AI42STaYAI",
    "outputId": "fa98977e-795c-4a04-ac99-5fad22db6130"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3DY0rDtaEAo",
    "outputId": "02cbf3d7-7f8a-4b0e-f683-c3e5f8766d43"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout \"Regression_Models.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jP6CVZJVgfTd"
   },
   "outputs": [],
   "source": [
    "import json, io\n",
    "\n",
    "in_path  = \"Regression_Models.ipynb\"\n",
    "out_path = \"Regression_Models.ipynb\"\n",
    "\n",
    "with io.open(in_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = json.load(f)\n",
    "\n",
    "# Remove top-level widgets metadata if present\n",
    "nb.get(\"metadata\", {}).pop(\"widgets\", None)\n",
    "\n",
    "# Remove any cell-level widgets metadata if present\n",
    "for cell in nb.get(\"cells\", []):\n",
    "    cell.get(\"metadata\", {}).pop(\"widgets\", None)\n",
    "\n",
    "with io.open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nb, f, ensure_ascii=False, indent=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "9cBua2SXaoDo",
    "outputId": "a9a29d41-c483-45f3-e213-41f8edda3d60"
   },
   "outputs": [],
   "source": [
    "files.download(\"Regression_Models.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbyqLdFkKZpZ"
   },
   "source": [
    "## Acknowledgements:\n",
    "Daphne Zhang, Jacob Boulrice, Shayla Schwartz, Yi Gao, Claude Sonnet 4, GPT-4o"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
